* 
* ==> Audit <==
* |------------|--------------------------------|----------|--------------------------------|---------|---------------------|---------------------|
|  Command   |              Args              | Profile  |              User              | Version |     Start Time      |      End Time       |
|------------|--------------------------------|----------|--------------------------------|---------|---------------------|---------------------|
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 09:16 EAT | 13 Nov 23 09:17 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 09:18 EAT | 13 Nov 23 09:20 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| docker-env |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 09:23 EAT | 13 Nov 23 09:23 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| docker-env | minikube docker-env --shell    | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 09:23 EAT | 13 Nov 23 09:23 EAT |
|            | cmd                            |          | Sani                           |         |                     |                     |
| ip         |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:14 EAT | 13 Nov 23 11:14 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| tunnel     |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:25 EAT | 13 Nov 23 11:27 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:28 EAT | 13 Nov 23 11:43 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| docker-env |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:47 EAT | 13 Nov 23 11:47 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:49 EAT | 13 Nov 23 11:50 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| docker-env |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:51 EAT | 13 Nov 23 11:51 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:51 EAT | 13 Nov 23 11:53 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 11:55 EAT | 13 Nov 23 15:37 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| dashboard  |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 13 Nov 23 15:37 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 14 Nov 23 16:50 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 20 Nov 23 22:09 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| ip         |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 20 Nov 23 22:09 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 20 Nov 23 23:28 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 20 Nov 23 23:29 EAT | 20 Nov 23 23:30 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 20 Nov 23 23:31 EAT | 20 Nov 23 23:32 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| ip         |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 10:52 EAT | 21 Nov 23 10:52 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 10:53 EAT | 21 Nov 23 10:54 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 10:54 EAT | 21 Nov 23 10:55 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-app-service --url      | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 10:56 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 10:56 EAT | 21 Nov 23 11:04 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 11:04 EAT | 21 Nov 23 21:26 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| stop       |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:26 EAT | 21 Nov 23 21:26 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:27 EAT | 21 Nov 23 21:28 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:28 EAT | 21 Nov 23 21:34 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| docker-env |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:36 EAT | 21 Nov 23 21:36 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:38 EAT | 21 Nov 23 21:40 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:40 EAT | 21 Nov 23 21:41 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:45 EAT | 21 Nov 23 21:49 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| docker-env |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:47 EAT | 21 Nov 23 21:47 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| docker-env | minikube docker-env --shell    | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:47 EAT | 21 Nov 23 21:47 EAT |
|            | cmd                            |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:49 EAT | 21 Nov 23 21:57 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| dashboard  |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 21 Nov 23 21:58 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:13 EAT | 22 Nov 23 13:14 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| stop       |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:14 EAT | 22 Nov 23 13:14 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:14 EAT | 22 Nov 23 13:15 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:18 EAT | 22 Nov 23 13:21 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| stop       |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:21 EAT | 22 Nov 23 13:21 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:21 EAT | 22 Nov 23 13:22 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:23 EAT | 22 Nov 23 13:32 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:34 EAT | 22 Nov 23 13:35 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:36 EAT | 22 Nov 23 13:37 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:37 EAT | 22 Nov 23 13:38 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 22 Nov 23 13:39 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 12:10 EAT | 23 Nov 23 12:12 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 12:13 EAT | 23 Nov 23 12:14 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 12:15 EAT | 23 Nov 23 17:19 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| stop       |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 17:19 EAT | 23 Nov 23 17:19 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| start      | --driver=docker                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 17:19 EAT | 23 Nov 23 17:20 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 17:21 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 17:24 EAT | 23 Nov 23 17:25 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 23 Nov 23 17:27 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| start      |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 11 Dec 23 18:14 EAT | 11 Dec 23 18:16 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 11 Dec 23 18:55 EAT | 11 Dec 23 18:59 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 11 Dec 23 19:00 EAT | 11 Dec 23 19:12 EAT |
|            |                                |          | Sani                           |         |                     |                     |
| dashboard  |                                | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 11 Dec 23 19:13 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
| service    | angular-service --url          | minikube | DESKTOP-04GOU7E\Abduljebar     | v1.32.0 | 11 Dec 23 19:32 EAT |                     |
|            |                                |          | Sani                           |         |                     |                     |
|------------|--------------------------------|----------|--------------------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/12/11 18:14:36
Running on machine: DESKTOP-04GOU7E
Binary: Built with gc go1.21.3 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1211 18:14:36.906889    7632 out.go:296] Setting OutFile to fd 84 ...
I1211 18:14:36.906889    7632 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1211 18:14:36.906889    7632 out.go:309] Setting ErrFile to fd 88...
I1211 18:14:36.906889    7632 out.go:343] TERM=,COLORTERM=, which probably does not support color
I1211 18:14:36.951933    7632 out.go:303] Setting JSON to false
I1211 18:14:36.959520    7632 start.go:128] hostinfo: {"hostname":"DESKTOP-04GOU7E","uptime":288860,"bootTime":1702018816,"procs":303,"os":"windows","platform":"Microsoft Windows 10 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.3758 Build 19045.3758","kernelVersion":"10.0.19045.3758 Build 19045.3758","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"56596aed-6733-4ff1-a284-fe973cbffca7"}
W1211 18:14:36.959520    7632 start.go:136] gopshost.Virtualization returned error: not implemented yet
I1211 18:14:36.961664    7632 out.go:177] * minikube v1.32.0 on Microsoft Windows 10 Home 10.0.19045.3758 Build 19045.3758
I1211 18:14:36.963522    7632 notify.go:220] Checking for updates...
I1211 18:14:36.966400    7632 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1211 18:14:36.968550    7632 driver.go:378] Setting default libvirt URI to qemu:///system
I1211 18:14:37.536715    7632 docker.go:122] docker version: linux-24.0.6:Docker Desktop 4.25.0 (126437)
I1211 18:14:37.538099    7632 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1211 18:14:40.286438    7632 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (2.7483389s)
I1211 18:14:40.287150    7632 info.go:266] docker info: {ID:4f54fefe-b43f-47b1-bfc0-b032dce4eeae Containers:79 ContainersRunning:58 ContainersPaused:0 ContainersStopped:21 Images:26 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:239 OomKillDisable:true NGoroutines:228 SystemTime:2023-12-11 15:14:40.218566608 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8259387392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:DESKTOP-04GOU7E Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I1211 18:14:40.290730    7632 out.go:177] * Using the docker driver based on existing profile
I1211 18:14:40.292497    7632 start.go:298] selected driver: docker
I1211 18:14:40.292497    7632 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Abduljebar Sani:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1211 18:14:40.292497    7632 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1211 18:14:40.319103    7632 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1211 18:14:41.169765    7632 info.go:266] docker info: {ID:4f54fefe-b43f-47b1-bfc0-b032dce4eeae Containers:79 ContainersRunning:58 ContainersPaused:0 ContainersStopped:21 Images:26 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:239 OomKillDisable:true NGoroutines:228 SystemTime:2023-12-11 15:14:41.118086357 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:8259387392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:DESKTOP-04GOU7E Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:8165feabfdfe38c65b599c4993d227328c231fca Expected:8165feabfdfe38c65b599c4993d227328c231fca} RuncCommit:{ID:v1.1.8-0-g82f18fe Expected:v1.1.8-0-g82f18fe} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2-desktop.5] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.0-desktop.1] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.20] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.9] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:C:\Program Files\Docker\cli-plugins\docker-scan.exe SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.0.9]] Warnings:<nil>}}
I1211 18:14:41.370242    7632 cni.go:84] Creating CNI manager for ""
I1211 18:14:41.370751    7632 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1211 18:14:41.370782    7632 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Abduljebar Sani:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1211 18:14:41.373279    7632 out.go:177] * Starting control plane node minikube in cluster minikube
I1211 18:14:41.374921    7632 cache.go:121] Beginning downloading kic base image for docker with docker
I1211 18:14:41.376087    7632 out.go:177] * Pulling base image ...
I1211 18:14:41.377793    7632 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1211 18:14:41.377793    7632 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I1211 18:14:41.718386    7632 preload.go:119] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.3/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
I1211 18:14:41.718386    7632 cache.go:56] Caching tarball of preloaded images
I1211 18:14:41.719179    7632 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1211 18:14:41.721747    7632 out.go:177] * Downloading Kubernetes v1.28.3 preload ...
I1211 18:14:41.727816    7632 preload.go:238] getting checksum for preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4 ...
I1211 18:14:41.735597    7632 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I1211 18:14:41.735756    7632 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I1211 18:14:42.398231    7632 download.go:107] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.3/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4?checksum=md5:82104bbf889ff8b69d5c141ce86c05ac -> C:\Users\Abduljebar Sani\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4
W1211 18:14:44.207338    7632 cache.go:62] Error downloading preloaded artifacts will continue without preload: download failed: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.28.3/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4?checksum=md5:82104bbf889ff8b69d5c141ce86c05ac: rename C:\Users\Abduljebar Sani\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4.download C:\Users\Abduljebar Sani\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.28.3-docker-overlay2-amd64.tar.lz4: Access is denied.
I1211 18:14:44.207338    7632 profile.go:148] Saving config to C:\Users\Abduljebar Sani\.minikube\profiles\minikube\config.json ...
I1211 18:14:44.207338    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner:v5 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner_v5
I1211 18:14:44.210326    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy_v1.28.3
I1211 18:14:44.210837    7632 cache.go:194] Successfully downloaded all kic artifacts
I1211 18:14:44.210837    7632 start.go:365] acquiring machines lock for minikube: {Name:mk1cc247a6fa7f56470e718aac52cfd8f2446983 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:44.210837    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager_v1.28.3
I1211 18:14:44.219611    7632 start.go:369] acquired machines lock for "minikube" in 8.2612ms
I1211 18:14:44.219611    7632 start.go:96] Skipping create...Using existing machine configuration
I1211 18:14:44.219611    7632 fix.go:54] fixHost starting: 
I1211 18:14:44.224486    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\pause:3.9 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\pause_3.9
I1211 18:14:44.224996    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver_v1.28.3
I1211 18:14:44.236777    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler_v1.28.3
I1211 18:14:44.244689    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\etcd:3.5.9-0 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\etcd_3.5.9-0
I1211 18:14:44.246120    7632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1211 18:14:44.246120    7632 localpath.go:146] windows sanitize: C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns:v1.10.1 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns_v1.10.1
I1211 18:14:46.132171    7632 cache.go:107] acquiring lock: {Name:mk62cb4ec8d7204128e28b8dda84fa8ed4397fc7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.140825    7632 cache.go:107] acquiring lock: {Name:mkaddda59974370a3a12630cd10515d1ba0be791 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.140825    7632 cache.go:107] acquiring lock: {Name:mkcae9d817685eef2bf2e6fa1a5fb78de33fce21 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.150021    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy_v1.28.3 exists
I1211 18:14:46.150352    7632 cache.go:96] cache image "registry.k8s.io/kube-proxy:v1.28.3" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-proxy_v1.28.3" took 1.9400263s
I1211 18:14:46.150352    7632 cache.go:80] save to tar file registry.k8s.io/kube-proxy:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-proxy_v1.28.3 succeeded
I1211 18:14:46.150352    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager_v1.28.3 exists
I1211 18:14:46.150864    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns_v1.10.1 exists
I1211 18:14:46.151073    7632 cache.go:96] cache image "registry.k8s.io/kube-controller-manager:v1.28.3" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-controller-manager_v1.28.3" took 1.9402354s
I1211 18:14:46.151073    7632 cache.go:80] save to tar file registry.k8s.io/kube-controller-manager:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-controller-manager_v1.28.3 succeeded
I1211 18:14:46.151073    7632 cache.go:107] acquiring lock: {Name:mk6f722da75c1395a0576f782a4b554d1d0bd73a Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.151073    7632 cache.go:96] cache image "registry.k8s.io/coredns/coredns:v1.10.1" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\coredns\\coredns_v1.10.1" took 1.9049525s
I1211 18:14:46.151073    7632 cache.go:80] save to tar file registry.k8s.io/coredns/coredns:v1.10.1 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\coredns\coredns_v1.10.1 succeeded
I1211 18:14:46.152286    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner_v5 exists
I1211 18:14:46.154103    7632 cache.go:96] cache image "gcr.io/k8s-minikube/storage-provisioner:v5" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\gcr.io\\k8s-minikube\\storage-provisioner_v5" took 1.9467654s
I1211 18:14:46.154103    7632 cache.go:80] save to tar file gcr.io/k8s-minikube/storage-provisioner:v5 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\gcr.io\k8s-minikube\storage-provisioner_v5 succeeded
I1211 18:14:46.161974    7632 cache.go:107] acquiring lock: {Name:mkcba161834637d0b10ba9f8002ff1fbbeec72fb Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.162482    7632 cache.go:107] acquiring lock: {Name:mkdefa0d1b600ebaad9593ec62585278aa0c86d7 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.164098    7632 cache.go:107] acquiring lock: {Name:mkbf4f1a2d1666fe96f5f5d64e84c6f852868ea1 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.164608    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\etcd_3.5.9-0 exists
I1211 18:14:46.173746    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver_v1.28.3 exists
I1211 18:14:46.173746    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler_v1.28.3 exists
I1211 18:14:46.175666    7632 cache.go:96] cache image "registry.k8s.io/etcd:3.5.9-0" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\etcd_3.5.9-0" took 1.9309765s
I1211 18:14:46.175666    7632 cache.go:80] save to tar file registry.k8s.io/etcd:3.5.9-0 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\etcd_3.5.9-0 succeeded
I1211 18:14:46.175666    7632 cache.go:96] cache image "registry.k8s.io/kube-apiserver:v1.28.3" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-apiserver_v1.28.3" took 1.9506696s
I1211 18:14:46.175666    7632 cache.go:80] save to tar file registry.k8s.io/kube-apiserver:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-apiserver_v1.28.3 succeeded
I1211 18:14:46.175666    7632 cache.go:107] acquiring lock: {Name:mkc402c0bd2543c7b1ab46201e17010db460e784 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1211 18:14:46.175666    7632 cache.go:96] cache image "registry.k8s.io/kube-scheduler:v1.28.3" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\kube-scheduler_v1.28.3" took 1.9388884s
I1211 18:14:46.175666    7632 cache.go:80] save to tar file registry.k8s.io/kube-scheduler:v1.28.3 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\kube-scheduler_v1.28.3 succeeded
I1211 18:14:46.176195    7632 cache.go:115] \\?\Volume{771a357d-ed1b-4146-817a-b44714fa1ef3}\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\pause_3.9 exists
I1211 18:14:46.176289    7632 cache.go:96] cache image "registry.k8s.io/pause:3.9" -> "C:\\Users\\Abduljebar Sani\\.minikube\\cache\\images\\amd64\\registry.k8s.io\\pause_3.9" took 1.9518031s
I1211 18:14:46.176289    7632 cache.go:80] save to tar file registry.k8s.io/pause:3.9 -> C:\Users\Abduljebar Sani\.minikube\cache\images\amd64\registry.k8s.io\pause_3.9 succeeded
I1211 18:14:46.176289    7632 cache.go:87] Successfully saved all images to host disk.
I1211 18:14:46.227030    7632 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.9809101s)
I1211 18:14:46.227030    7632 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1211 18:14:46.227030    7632 fix.go:128] unexpected machine state, will restart: <nil>
I1211 18:14:46.230378    7632 out.go:177] * Restarting existing docker container for "minikube" ...
I1211 18:14:46.242656    7632 cli_runner.go:164] Run: docker start minikube
I1211 18:14:47.657105    7632 cli_runner.go:217] Completed: docker start minikube: (1.4112406s)
I1211 18:14:47.670348    7632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1211 18:14:48.070208    7632 kic.go:430] container "minikube" state is running.
I1211 18:14:48.089413    7632 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1211 18:14:48.472637    7632 profile.go:148] Saving config to C:\Users\Abduljebar Sani\.minikube\profiles\minikube\config.json ...
I1211 18:14:48.474302    7632 machine.go:88] provisioning docker machine ...
I1211 18:14:48.474302    7632 ubuntu.go:169] provisioning hostname "minikube"
I1211 18:14:48.482712    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:48.877875    7632 main.go:141] libmachine: Using SSH client type: native
I1211 18:14:48.880071    7632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7847e0] 0x787320 <nil>  [] 0s} 127.0.0.1 2488 <nil> <nil>}
I1211 18:14:48.880071    7632 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1211 18:14:48.926702    7632 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1211 18:14:52.470969    7632 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1211 18:14:52.483468    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:52.995295    7632 main.go:141] libmachine: Using SSH client type: native
I1211 18:14:52.996425    7632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7847e0] 0x787320 <nil>  [] 0s} 127.0.0.1 2488 <nil> <nil>}
I1211 18:14:52.996425    7632 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1211 18:14:53.502149    7632 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1211 18:14:53.503678    7632 ubuntu.go:175] set auth options {CertDir:C:\Users\Abduljebar Sani\.minikube CaCertPath:C:\Users\Abduljebar Sani\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Abduljebar Sani\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Abduljebar Sani\.minikube\machines\server.pem ServerKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Abduljebar Sani\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Abduljebar Sani\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Abduljebar Sani\.minikube}
I1211 18:14:53.503678    7632 ubuntu.go:177] setting up certificates
I1211 18:14:53.504200    7632 provision.go:83] configureAuth start
I1211 18:14:53.516861    7632 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1211 18:14:54.006626    7632 provision.go:138] copyHostCerts
I1211 18:14:54.011110    7632 exec_runner.go:144] found C:\Users\Abduljebar Sani\.minikube/ca.pem, removing ...
I1211 18:14:54.011735    7632 exec_runner.go:203] rm: C:\Users\Abduljebar Sani\.minikube\ca.pem
I1211 18:14:54.012959    7632 exec_runner.go:151] cp: C:\Users\Abduljebar Sani\.minikube\certs\ca.pem --> C:\Users\Abduljebar Sani\.minikube/ca.pem (1103 bytes)
I1211 18:14:54.017055    7632 exec_runner.go:144] found C:\Users\Abduljebar Sani\.minikube/cert.pem, removing ...
I1211 18:14:54.017055    7632 exec_runner.go:203] rm: C:\Users\Abduljebar Sani\.minikube\cert.pem
I1211 18:14:54.017682    7632 exec_runner.go:151] cp: C:\Users\Abduljebar Sani\.minikube\certs\cert.pem --> C:\Users\Abduljebar Sani\.minikube/cert.pem (1147 bytes)
I1211 18:14:54.019282    7632 exec_runner.go:144] found C:\Users\Abduljebar Sani\.minikube/key.pem, removing ...
I1211 18:14:54.019282    7632 exec_runner.go:203] rm: C:\Users\Abduljebar Sani\.minikube\key.pem
I1211 18:14:54.019817    7632 exec_runner.go:151] cp: C:\Users\Abduljebar Sani\.minikube\certs\key.pem --> C:\Users\Abduljebar Sani\.minikube/key.pem (1675 bytes)
I1211 18:14:54.021336    7632 provision.go:112] generating server cert: C:\Users\Abduljebar Sani\.minikube\machines\server.pem ca-key=C:\Users\Abduljebar Sani\.minikube\certs\ca.pem private-key=C:\Users\Abduljebar Sani\.minikube\certs\ca-key.pem org=Abduljebar Sani.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1211 18:14:54.526222    7632 provision.go:172] copyRemoteCerts
I1211 18:14:54.556603    7632 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1211 18:14:54.578358    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:55.027553    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:14:55.213017    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1103 bytes)
I1211 18:14:55.408860    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\machines\server.pem --> /etc/docker/server.pem (1224 bytes)
I1211 18:14:55.520945    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1211 18:14:55.642018    7632 provision.go:86] duration metric: configureAuth took 2.1372922s
I1211 18:14:55.642018    7632 ubuntu.go:193] setting minikube options for container-runtime
I1211 18:14:55.643643    7632 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1211 18:14:55.652602    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:55.990415    7632 main.go:141] libmachine: Using SSH client type: native
I1211 18:14:55.991001    7632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7847e0] 0x787320 <nil>  [] 0s} 127.0.0.1 2488 <nil> <nil>}
I1211 18:14:55.991001    7632 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1211 18:14:56.163293    7632 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1211 18:14:56.163293    7632 ubuntu.go:71] root file system type: overlay
I1211 18:14:56.164206    7632 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1211 18:14:56.175443    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:56.462452    7632 main.go:141] libmachine: Using SSH client type: native
I1211 18:14:56.462987    7632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7847e0] 0x787320 <nil>  [] 0s} 127.0.0.1 2488 <nil> <nil>}
I1211 18:14:56.462987    7632 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1211 18:14:56.689949    7632 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1211 18:14:56.701478    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:56.947145    7632 main.go:141] libmachine: Using SSH client type: native
I1211 18:14:56.947682    7632 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x7847e0] 0x787320 <nil>  [] 0s} 127.0.0.1 2488 <nil> <nil>}
I1211 18:14:56.947682    7632 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1211 18:14:57.130214    7632 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1211 18:14:57.130214    7632 machine.go:91] provisioned docker machine in 8.655912s
I1211 18:14:57.131488    7632 start.go:300] post-start starting for "minikube" (driver="docker")
I1211 18:14:57.131488    7632 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1211 18:14:57.162568    7632 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1211 18:14:57.169485    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:57.456615    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:14:57.619209    7632 ssh_runner.go:195] Run: cat /etc/os-release
I1211 18:14:57.630570    7632 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1211 18:14:57.630570    7632 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1211 18:14:57.630570    7632 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1211 18:14:57.630570    7632 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I1211 18:14:57.631298    7632 filesync.go:126] Scanning C:\Users\Abduljebar Sani\.minikube\addons for local assets ...
I1211 18:14:57.631848    7632 filesync.go:126] Scanning C:\Users\Abduljebar Sani\.minikube\files for local assets ...
I1211 18:14:57.631848    7632 start.go:303] post-start completed in 500.3599ms
I1211 18:14:57.638357    7632 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1211 18:14:57.649987    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:57.936411    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:14:58.066703    7632 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1211 18:14:58.077786    7632 fix.go:56] fixHost completed within 13.8581756s
I1211 18:14:58.077786    7632 start.go:83] releasing machines lock for "minikube", held for 13.8581756s
I1211 18:14:58.089085    7632 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1211 18:14:58.426351    7632 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1211 18:14:58.429040    7632 ssh_runner.go:195] Run: cat /version.json
I1211 18:14:58.437329    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:58.437466    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:14:58.810701    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:14:58.851254    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:15:00.149930    7632 ssh_runner.go:235] Completed: cat /version.json: (1.7208908s)
I1211 18:15:00.149930    7632 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (1.7235796s)
W1211 18:15:00.149930    7632 start.go:840] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 7
stdout:

stderr:
curl: (7) Couldn't connect to server
W1211 18:15:00.151096    7632 out.go:239] ! This container is having trouble accessing https://registry.k8s.io
W1211 18:15:00.158266    7632 out.go:239] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1211 18:15:00.192744    7632 ssh_runner.go:195] Run: systemctl --version
I1211 18:15:00.243186    7632 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1211 18:15:00.285543    7632 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1211 18:15:00.337473    7632 start.go:416] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1211 18:15:00.364741    7632 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1211 18:15:00.394107    7632 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1211 18:15:00.394107    7632 start.go:472] detecting cgroup driver to use...
I1211 18:15:00.394107    7632 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1211 18:15:00.397362    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1211 18:15:00.451470    7632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1211 18:15:00.486906    7632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1211 18:15:00.517395    7632 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I1211 18:15:00.526128    7632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1211 18:15:00.563937    7632 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1211 18:15:00.599621    7632 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1211 18:15:00.640874    7632 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1211 18:15:00.677347    7632 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1211 18:15:00.712891    7632 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1211 18:15:00.766333    7632 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1211 18:15:00.821732    7632 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1211 18:15:00.887303    7632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1211 18:15:01.084321    7632 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1211 18:15:01.260645    7632 start.go:472] detecting cgroup driver to use...
I1211 18:15:01.260645    7632 detect.go:196] detected "cgroupfs" cgroup driver on host os
I1211 18:15:01.287848    7632 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1211 18:15:01.330699    7632 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1211 18:15:01.357217    7632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1211 18:15:01.421744    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1211 18:15:01.502135    7632 ssh_runner.go:195] Run: which cri-dockerd
I1211 18:15:01.543785    7632 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1211 18:15:01.590701    7632 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1211 18:15:01.692440    7632 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1211 18:15:01.974600    7632 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1211 18:15:02.150542    7632 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I1211 18:15:02.150752    7632 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1211 18:15:02.230128    7632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1211 18:15:02.443337    7632 ssh_runner.go:195] Run: sudo systemctl restart docker
I1211 18:15:03.327055    7632 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1211 18:15:03.525370    7632 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1211 18:15:03.723994    7632 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1211 18:15:03.928656    7632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1211 18:15:04.117765    7632 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1211 18:15:04.181789    7632 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1211 18:15:04.380870    7632 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1211 18:15:05.016663    7632 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1211 18:15:05.026162    7632 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1211 18:15:05.038402    7632 start.go:540] Will wait 60s for crictl version
I1211 18:15:05.045829    7632 ssh_runner.go:195] Run: which crictl
I1211 18:15:05.073447    7632 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1211 18:15:05.407739    7632 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I1211 18:15:05.424575    7632 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1211 18:15:05.680019    7632 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1211 18:15:05.736866    7632 out.go:204] * Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I1211 18:15:05.749108    7632 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1211 18:15:06.426781    7632 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1211 18:15:06.432688    7632 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1211 18:15:06.443166    7632 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1211 18:15:06.487665    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1211 18:15:06.824352    7632 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I1211 18:15:06.848084    7632 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1211 18:15:06.900276    7632 docker.go:671] Got preloaded images: -- stdout --
naima68/angular-project:latest
naima68/angular-project:v1.0
<none>:<none>
naima68/angular-project:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1211 18:15:06.902081    7632 docker.go:601] Images already preloaded, skipping extraction
I1211 18:15:06.917094    7632 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1211 18:15:06.990772    7632 docker.go:671] Got preloaded images: -- stdout --
naima68/angular-project:latest
naima68/angular-project:v1.0
<none>:<none>
naima68/angular-project:<none>
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1211 18:15:06.990772    7632 cache_images.go:84] Images are preloaded, skipping loading
I1211 18:15:07.003872    7632 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1211 18:15:07.620196    7632 cni.go:84] Creating CNI manager for ""
I1211 18:15:07.628118    7632 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1211 18:15:07.629251    7632 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1211 18:15:07.629251    7632 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1211 18:15:07.630818    7632 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1211 18:15:07.631883    7632 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1211 18:15:07.668122    7632 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I1211 18:15:07.741903    7632 binaries.go:44] Found k8s binaries, skipping transfer
I1211 18:15:07.769581    7632 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1211 18:15:07.836693    7632 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1211 18:15:07.956788    7632 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1211 18:15:08.094817    7632 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I1211 18:15:08.187207    7632 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1211 18:15:08.200003    7632 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1211 18:15:08.234736    7632 certs.go:56] Setting up C:\Users\Abduljebar Sani\.minikube\profiles\minikube for IP: 192.168.49.2
I1211 18:15:08.234736    7632 certs.go:190] acquiring lock for shared ca certs: {Name:mk3699e3cedacaf9c2639ed88d97064bc261f45d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1211 18:15:08.236923    7632 certs.go:199] skipping minikubeCA CA generation: C:\Users\Abduljebar Sani\.minikube\ca.key
I1211 18:15:08.238044    7632 certs.go:199] skipping proxyClientCA CA generation: C:\Users\Abduljebar Sani\.minikube\proxy-client-ca.key
I1211 18:15:08.241802    7632 certs.go:315] skipping minikube-user signed cert generation: C:\Users\Abduljebar Sani\.minikube\profiles\minikube\client.key
I1211 18:15:08.242892    7632 certs.go:315] skipping minikube signed cert generation: C:\Users\Abduljebar Sani\.minikube\profiles\minikube\apiserver.key.dd3b5fb2
I1211 18:15:08.244055    7632 certs.go:315] skipping aggregator signed cert generation: C:\Users\Abduljebar Sani\.minikube\profiles\minikube\proxy-client.key
I1211 18:15:08.246402    7632 certs.go:437] found cert: C:\Users\Abduljebar Sani\.minikube\certs\C:\Users\Abduljebar Sani\.minikube\certs\ca-key.pem (1679 bytes)
I1211 18:15:08.246909    7632 certs.go:437] found cert: C:\Users\Abduljebar Sani\.minikube\certs\C:\Users\Abduljebar Sani\.minikube\certs\ca.pem (1103 bytes)
I1211 18:15:08.247114    7632 certs.go:437] found cert: C:\Users\Abduljebar Sani\.minikube\certs\C:\Users\Abduljebar Sani\.minikube\certs\cert.pem (1147 bytes)
I1211 18:15:08.247655    7632 certs.go:437] found cert: C:\Users\Abduljebar Sani\.minikube\certs\C:\Users\Abduljebar Sani\.minikube\certs\key.pem (1675 bytes)
I1211 18:15:08.256729    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1211 18:15:08.364828    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1211 18:15:08.462340    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1211 18:15:08.599785    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1211 18:15:08.742416    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1211 18:15:08.902927    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1211 18:15:09.071306    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1211 18:15:09.183398    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1211 18:15:09.275999    7632 ssh_runner.go:362] scp C:\Users\Abduljebar Sani\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1211 18:15:09.392898    7632 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1211 18:15:09.481081    7632 ssh_runner.go:195] Run: openssl version
I1211 18:15:09.548615    7632 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1211 18:15:09.612077    7632 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1211 18:15:09.624146    7632 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Nov 12 16:46 /usr/share/ca-certificates/minikubeCA.pem
I1211 18:15:09.633033    7632 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1211 18:15:09.695008    7632 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1211 18:15:09.762918    7632 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1211 18:15:09.796623    7632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1211 18:15:09.858440    7632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1211 18:15:09.894954    7632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1211 18:15:09.931718    7632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1211 18:15:09.968523    7632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1211 18:15:10.018941    7632 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1211 18:15:10.059652    7632 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Abduljebar Sani:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I1211 18:15:10.070827    7632 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1211 18:15:10.191908    7632 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1211 18:15:10.224932    7632 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1211 18:15:10.224932    7632 kubeadm.go:636] restartCluster start
I1211 18:15:10.250199    7632 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1211 18:15:10.293816    7632 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1211 18:15:10.303928    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1211 18:15:10.681133    7632 kubeconfig.go:92] found "minikube" server: "https://127.0.0.1:50659"
I1211 18:15:10.681803    7632 kubeconfig.go:135] verify returned: got: 127.0.0.1:50659, want: 127.0.0.1:2492
I1211 18:15:10.686075    7632 lock.go:35] WriteFile acquiring C:\Users\Abduljebar Sani\.kube\config: {Name:mk55e6ac38c826f41590546f941c98d640246a72 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1211 18:15:10.741713    7632 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1211 18:15:10.865116    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:10.887909    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:10.991749    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:10.991749    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:11.027101    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:11.084767    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:11.596285    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:11.613036    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:11.648461    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:12.095409    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:12.120641    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:12.158127    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:12.595849    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:12.662630    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:12.759614    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:13.089823    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:13.109445    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:13.148872    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:13.601491    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:13.737895    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:13.774378    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:14.096131    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:14.143148    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:14.230319    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:14.593149    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:14.618914    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:14.688137    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:15.099312    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:15.117833    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:15.147616    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:15.594103    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:15.621888    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:15.685442    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:16.099757    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:16.125445    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:16.157157    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:16.592224    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:16.616843    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:16.651922    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:17.095233    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:17.113491    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:17.143525    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:17.590507    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:17.611520    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:17.643867    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:18.105812    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:18.126916    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:18.158146    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:18.592210    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:18.611080    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:18.643452    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:19.093337    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:19.112770    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:19.143138    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:19.585864    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:19.612188    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:19.673663    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:20.102949    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:20.123444    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:20.160043    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:20.588146    7632 api_server.go:166] Checking apiserver status ...
I1211 18:15:20.613373    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1211 18:15:20.658738    7632 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1211 18:15:20.865519    7632 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1211 18:15:20.865519    7632 kubeadm.go:1128] stopping kube-system containers ...
I1211 18:15:20.878675    7632 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1211 18:15:20.923602    7632 docker.go:469] Stopping containers: [43ee0899fd05 3f9fee803747 9894f2cb3ae2 2dc6015afaa7 669cf8ca44a0 72b7c5030eb8 461f2ebcf0b4 0fe1e69c866a 05892b1e090c c092c5e4a572 663d93960dae c2a17317da76 fc5d7303e321 8ac78f467cb6 96bd6b99c775 d6d8e8b4d4ec bef846d5ce35 6b88956cf335 fd72dac596ac 0f5c43024198 ba3e46f03dec 1992ce36dc6f 386edc65e3f6 2ff9196c1f5d 7aac52f69110 51612e2614c6 97b7b4445c77]
I1211 18:15:20.932328    7632 ssh_runner.go:195] Run: docker stop 43ee0899fd05 3f9fee803747 9894f2cb3ae2 2dc6015afaa7 669cf8ca44a0 72b7c5030eb8 461f2ebcf0b4 0fe1e69c866a 05892b1e090c c092c5e4a572 663d93960dae c2a17317da76 fc5d7303e321 8ac78f467cb6 96bd6b99c775 d6d8e8b4d4ec bef846d5ce35 6b88956cf335 fd72dac596ac 0f5c43024198 ba3e46f03dec 1992ce36dc6f 386edc65e3f6 2ff9196c1f5d 7aac52f69110 51612e2614c6 97b7b4445c77
I1211 18:15:21.000370    7632 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1211 18:15:21.053767    7632 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1211 18:15:21.085317    7632 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5639 Nov 12 16:46 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Nov 23 14:20 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Nov 12 16:46 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Nov 23 14:20 /etc/kubernetes/scheduler.conf

I1211 18:15:21.110812    7632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1211 18:15:21.164212    7632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1211 18:15:21.217678    7632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1211 18:15:21.246372    7632 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1211 18:15:21.271168    7632 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1211 18:15:21.323705    7632 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1211 18:15:21.351286    7632 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1211 18:15:21.371326    7632 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1211 18:15:21.431952    7632 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1211 18:15:21.464199    7632 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1211 18:15:21.464199    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1211 18:15:21.887226    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1211 18:15:23.237756    7632 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.3505297s)
I1211 18:15:23.237756    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1211 18:15:23.641693    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1211 18:15:23.819610    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1211 18:15:23.987159    7632 api_server.go:52] waiting for apiserver process to appear ...
I1211 18:15:24.021726    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:24.085774    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:24.659900    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:25.194348    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:25.687684    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:26.187152    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:26.664299    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:27.177391    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:27.659995    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:28.164402    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:28.669576    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:29.160132    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:15:29.203339    7632 api_server.go:72] duration metric: took 5.21618s to wait for apiserver process to appear ...
I1211 18:15:29.203339    7632 api_server.go:88] waiting for apiserver healthz status ...
I1211 18:15:29.203857    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:29.210915    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:29.210915    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:29.215402    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:29.717196    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:29.721355    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:30.226196    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:30.230051    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:30.720765    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:30.729729    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:31.220966    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:31.227432    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:31.718535    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:31.725552    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:32.220726    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:32.226392    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:32.725006    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:32.729607    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:33.226665    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:33.231811    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:33.721512    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:33.728541    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:34.227041    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:34.232286    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:34.723342    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:34.732004    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:35.225891    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:35.230442    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:35.717706    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:35.722442    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:36.227790    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:36.233512    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:36.716775    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:36.722241    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:37.224081    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:37.229366    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:37.726425    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:37.732528    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:38.231400    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:38.244218    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:38.729007    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:38.734782    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:39.235288    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:39.274990    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:39.717939    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:39.726127    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:40.224911    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:40.237711    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:40.732846    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:40.750636    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": EOF
I1211 18:15:41.229988    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:46.232817    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1211 18:15:46.232817    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:51.235940    7632 api_server.go:269] stopped: https://127.0.0.1:2492/healthz: Get "https://127.0.0.1:2492/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1211 18:15:51.235940    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:55.325071    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:55.326797    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:15:55.326797    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:55.660959    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:55.660959    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:15:55.726454    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:55.997486    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:55.997486    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:15:56.232260    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:57.728401    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:57.728401    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:15:57.728401    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:58.125738    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:58.125738    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:15:58.125738    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:59.331334    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:59.331334    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:15:59.331334    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:59.567555    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:59.569065    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:15:59.747403    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:15:59.966114    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:15:59.966644    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:00.225666    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:00.384055    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:00.384055    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:00.730857    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:00.877757    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:00.877757    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:01.230442    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:01.397459    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:01.397459    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:01.726622    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:01.958200    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:01.958200    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:02.219691    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:02.746948    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:02.746948    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:02.746948    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:02.893894    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:02.894420    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:03.230032    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:03.577464    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:03.577464    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:03.732591    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:03.831785    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:03.831785    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:04.232848    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:04.277134    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:04.277134    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:04.724746    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:04.759788    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:04.760300    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:05.227645    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:05.376469    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:05.376469    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:05.722712    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:05.978463    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:05.978463    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:06.223079    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:06.875718    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:06.876243    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:06.876243    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:06.983166    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1211 18:16:06.983166    7632 api_server.go:103] status: https://127.0.0.1:2492/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1211 18:16:07.216592    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:07.289024    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 200:
ok
I1211 18:16:07.391624    7632 api_server.go:141] control plane version: v1.28.3
I1211 18:16:07.391624    7632 api_server.go:131] duration metric: took 38.1882856s to wait for apiserver health ...
I1211 18:16:07.391624    7632 cni.go:84] Creating CNI manager for ""
I1211 18:16:07.391624    7632 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1211 18:16:07.392722    7632 out.go:177] * Configuring bridge CNI (Container Networking Interface) ...
I1211 18:16:07.438520    7632 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1211 18:16:07.959079    7632 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1211 18:16:08.424368    7632 system_pods.go:43] waiting for kube-system pods to appear ...
I1211 18:16:08.503804    7632 system_pods.go:59] 7 kube-system pods found
I1211 18:16:08.503804    7632 system_pods.go:61] "coredns-5dd5756b68-lsrvg" [52a7a086-ab55-4c15-85bc-f59cc40dc807] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1211 18:16:08.503804    7632 system_pods.go:61] "etcd-minikube" [a8756826-656b-4d90-9e61-115b1ab68b84] Running
I1211 18:16:08.503804    7632 system_pods.go:61] "kube-apiserver-minikube" [ed0d0cd5-2dd1-4ec7-9926-e0061f452733] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1211 18:16:08.503804    7632 system_pods.go:61] "kube-controller-manager-minikube" [b1353382-71e7-4aff-ac23-8493fe3dc4c8] Running
I1211 18:16:08.503804    7632 system_pods.go:61] "kube-proxy-vvzfd" [b36a3529-d299-4abf-92c1-80b5a72f9255] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1211 18:16:08.503804    7632 system_pods.go:61] "kube-scheduler-minikube" [7506ca4b-24e2-42a6-90b0-9e858322af6d] Running
I1211 18:16:08.503804    7632 system_pods.go:61] "storage-provisioner" [0eef51fb-db29-4049-bb68-e9c67dfe9b83] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1211 18:16:08.503804    7632 system_pods.go:74] duration metric: took 79.4363ms to wait for pod list to return data ...
I1211 18:16:08.503804    7632 node_conditions.go:102] verifying NodePressure condition ...
I1211 18:16:08.519601    7632 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1211 18:16:08.519601    7632 node_conditions.go:123] node cpu capacity is 4
I1211 18:16:08.521424    7632 node_conditions.go:105] duration metric: took 17.6204ms to run NodePressure ...
I1211 18:16:08.521696    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1211 18:16:10.115327    7632 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (1.5936308s)
I1211 18:16:10.115327    7632 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1211 18:16:10.180960    7632 ops.go:34] apiserver oom_adj: -16
I1211 18:16:10.181479    7632 kubeadm.go:640] restartCluster took 59.9565355s
I1211 18:16:10.181479    7632 kubeadm.go:406] StartCluster complete in 1m0.1218266s
I1211 18:16:10.182753    7632 settings.go:142] acquiring lock: {Name:mk703101c14495bd13f0b3cc7b80d67bc467ef08 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1211 18:16:10.182753    7632 settings.go:150] Updating kubeconfig:  C:\Users\Abduljebar Sani\.kube\config
I1211 18:16:10.187259    7632 lock.go:35] WriteFile acquiring C:\Users\Abduljebar Sani\.kube\config: {Name:mk55e6ac38c826f41590546f941c98d640246a72 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1211 18:16:10.195271    7632 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I1211 18:16:10.197562    7632 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1211 18:16:10.203929    7632 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I1211 18:16:10.205644    7632 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1211 18:16:10.205644    7632 addons.go:231] Setting addon storage-provisioner=true in "minikube"
W1211 18:16:10.205644    7632 addons.go:240] addon storage-provisioner should already be in state true
I1211 18:16:10.205644    7632 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1211 18:16:10.205644    7632 addons.go:69] Setting dashboard=true in profile "minikube"
I1211 18:16:10.205644    7632 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1211 18:16:10.205644    7632 addons.go:231] Setting addon dashboard=true in "minikube"
W1211 18:16:10.205644    7632 addons.go:240] addon dashboard should already be in state true
I1211 18:16:10.217702    7632 host.go:66] Checking if "minikube" exists ...
I1211 18:16:10.218184    7632 host.go:66] Checking if "minikube" exists ...
I1211 18:16:10.266706    7632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1211 18:16:10.280147    7632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1211 18:16:10.281695    7632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1211 18:16:10.421937    7632 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1211 18:16:10.425576    7632 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I1211 18:16:10.426670    7632 out.go:177] * Verifying Kubernetes components...
I1211 18:16:10.467190    7632 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1211 18:16:11.397507    7632 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.1168349s)
I1211 18:16:11.408080    7632 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1211 18:16:11.426065    7632 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1211 18:16:11.426065    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1211 18:16:11.442919    7632 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.1612243s)
I1211 18:16:11.442919    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:16:11.442919    7632 addons.go:231] Setting addon default-storageclass=true in "minikube"
W1211 18:16:11.442919    7632 addons.go:240] addon default-storageclass should already be in state true
I1211 18:16:11.442919    7632 host.go:66] Checking if "minikube" exists ...
I1211 18:16:11.475126    7632 cli_runner.go:217] Completed: docker container inspect minikube --format={{.State.Status}}: (1.2038605s)
I1211 18:16:11.494167    7632 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I1211 18:16:11.486691    7632 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1211 18:16:11.570823    7632 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1211 18:16:11.594963    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1211 18:16:11.594963    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1211 18:16:11.610602    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:16:12.478135    7632 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1211 18:16:12.478135    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1211 18:16:12.492026    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1211 18:16:12.588259    7632 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.1453401s)
I1211 18:16:12.588259    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:16:12.645201    7632 cli_runner.go:217] Completed: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube: (1.0345982s)
I1211 18:16:12.645201    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:16:13.172773    7632 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:2488 SSHKeyPath:C:\Users\Abduljebar Sani\.minikube\machines\minikube\id_rsa Username:docker}
I1211 18:16:14.296123    7632 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1211 18:16:14.300923    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1211 18:16:14.300923    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1211 18:16:14.754405    7632 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1211 18:16:15.316223    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1211 18:16:15.316239    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1211 18:16:16.185703    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1211 18:16:16.186224    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1211 18:16:16.600894    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1211 18:16:16.600894    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1211 18:16:16.970629    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-role.yaml
I1211 18:16:16.970629    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1211 18:16:17.773778    7632 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (7.5751893s)
I1211 18:16:17.773778    7632 start.go:899] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1211 18:16:17.773778    7632 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (7.3065879s)
I1211 18:16:17.789181    7632 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1211 18:16:18.038583    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1211 18:16:18.038583    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1211 18:16:18.103224    7632 api_server.go:52] waiting for apiserver process to appear ...
I1211 18:16:18.134840    7632 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1211 18:16:18.459220    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1211 18:16:18.459220    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1211 18:16:18.965213    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1211 18:16:18.965213    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1211 18:16:19.393959    7632 addons.go:423] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1211 18:16:19.393959    7632 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1211 18:16:19.834197    7632 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1211 18:16:26.164090    7632 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (11.4096844s)
I1211 18:16:26.164090    7632 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (11.8679671s)
I1211 18:16:26.164090    7632 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (8.02925s)
I1211 18:16:26.164090    7632 api_server.go:72] duration metric: took 15.738514s to wait for apiserver process to appear ...
I1211 18:16:26.164090    7632 api_server.go:88] waiting for apiserver healthz status ...
I1211 18:16:26.164090    7632 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:2492/healthz ...
I1211 18:16:26.270974    7632 api_server.go:279] https://127.0.0.1:2492/healthz returned 200:
ok
I1211 18:16:26.299385    7632 api_server.go:141] control plane version: v1.28.3
I1211 18:16:26.299385    7632 api_server.go:131] duration metric: took 135.2954ms to wait for apiserver health ...
I1211 18:16:26.299385    7632 system_pods.go:43] waiting for kube-system pods to appear ...
I1211 18:16:26.362827    7632 system_pods.go:59] 7 kube-system pods found
I1211 18:16:26.362827    7632 system_pods.go:61] "coredns-5dd5756b68-lsrvg" [52a7a086-ab55-4c15-85bc-f59cc40dc807] Running
I1211 18:16:26.362827    7632 system_pods.go:61] "etcd-minikube" [a8756826-656b-4d90-9e61-115b1ab68b84] Running
I1211 18:16:26.362827    7632 system_pods.go:61] "kube-apiserver-minikube" [ed0d0cd5-2dd1-4ec7-9926-e0061f452733] Running
I1211 18:16:26.362827    7632 system_pods.go:61] "kube-controller-manager-minikube" [b1353382-71e7-4aff-ac23-8493fe3dc4c8] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1211 18:16:26.362827    7632 system_pods.go:61] "kube-proxy-vvzfd" [b36a3529-d299-4abf-92c1-80b5a72f9255] Running
I1211 18:16:26.362827    7632 system_pods.go:61] "kube-scheduler-minikube" [7506ca4b-24e2-42a6-90b0-9e858322af6d] Running
I1211 18:16:26.362827    7632 system_pods.go:61] "storage-provisioner" [0eef51fb-db29-4049-bb68-e9c67dfe9b83] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1211 18:16:26.362827    7632 system_pods.go:74] duration metric: took 63.4417ms to wait for pod list to return data ...
I1211 18:16:26.362827    7632 kubeadm.go:581] duration metric: took 15.9372511s to wait for : map[apiserver:true system_pods:true] ...
I1211 18:16:26.362827    7632 node_conditions.go:102] verifying NodePressure condition ...
I1211 18:16:26.428694    7632 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1211 18:16:26.428694    7632 node_conditions.go:123] node cpu capacity is 4
I1211 18:16:26.428694    7632 node_conditions.go:105] duration metric: took 65.8669ms to run NodePressure ...
I1211 18:16:26.428694    7632 start.go:228] waiting for startup goroutines ...
I1211 18:16:29.618049    7632 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (9.783327s)
I1211 18:16:29.619804    7632 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server	


I1211 18:16:29.645369    7632 out.go:177] * Enabled addons: storage-provisioner, default-storageclass, dashboard
I1211 18:16:29.650308    7632 addons.go:502] enable addons completed in 19.4527465s: enabled=[storage-provisioner default-storageclass dashboard]
I1211 18:16:29.650308    7632 start.go:233] waiting for cluster config update ...
I1211 18:16:29.650308    7632 start.go:242] writing updated cluster config ...
I1211 18:16:29.667623    7632 ssh_runner.go:195] Run: rm -f paused
I1211 18:16:30.413065    7632 start.go:600] kubectl: 1.28.2, cluster: 1.28.3 (minor skew: 0)
I1211 18:16:30.414726    7632 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Dec 11 15:15:04 minikube cri-dockerd[1196]: time="2023-12-11T15:15:04Z" level=info msg="Start docker client with request timeout 0s"
Dec 11 15:15:04 minikube cri-dockerd[1196]: time="2023-12-11T15:15:04Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 11 15:15:04 minikube cri-dockerd[1196]: time="2023-12-11T15:15:04Z" level=info msg="Loaded network plugin cni"
Dec 11 15:15:04 minikube cri-dockerd[1196]: time="2023-12-11T15:15:04Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 11 15:15:05 minikube cri-dockerd[1196]: time="2023-12-11T15:15:05Z" level=info msg="Docker Info: &{ID:ef1a78d1-ce83-4ccb-8531-7fc6d4756c7b Containers:38 ContainersRunning:0 ContainersPaused:0 ContainersStopped:38 Images:14 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:25 OomKillDisable:true NGoroutines:36 SystemTime:2023-12-11T15:15:04.992010559Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:1 NEventsListener:0 KernelVersion:5.15.90.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.3 LTS (containerized) OSVersion:22.04 OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0xc0001cef50 NCPU:4 MemTotal:8259387392 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: DefaultAddressPools:[] Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support]}"
Dec 11 15:15:05 minikube cri-dockerd[1196]: time="2023-12-11T15:15:05Z" level=info msg="Setting cgroupDriver cgroupfs"
Dec 11 15:15:05 minikube cri-dockerd[1196]: time="2023-12-11T15:15:05Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 11 15:15:05 minikube cri-dockerd[1196]: time="2023-12-11T15:15:05Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 11 15:15:05 minikube cri-dockerd[1196]: time="2023-12-11T15:15:05Z" level=info msg="Start cri-dockerd grpc backend"
Dec 11 15:15:05 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 11 15:15:25 minikube cri-dockerd[1196]: time="2023-12-11T15:15:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-6ttp9_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f4c305472f21ee1d58349f31e8ef5c9d7bbc9212c7fb4a33165d560d2e7e7e75\""
Dec 11 15:15:25 minikube cri-dockerd[1196]: time="2023-12-11T15:15:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-lsrvg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"05892b1e090c9f4b47efc9e634d0749cabfc747c64e7783e31256c57aecc77e7\""
Dec 11 15:15:25 minikube cri-dockerd[1196]: time="2023-12-11T15:15:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-lsrvg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6b88956cf33560d55bb08627e9673d16eb13e2d390aa1c5eda3e5d964d188379\""
Dec 11 15:15:25 minikube cri-dockerd[1196]: time="2023-12-11T15:15:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"angular-deployment-657b4d6b7b-4zbsx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"608e92bd9f23f1db5031aecc4fb870090671e288fcabeb7eb509cf2c6bc3a6bb\""
Dec 11 15:15:25 minikube cri-dockerd[1196]: time="2023-12-11T15:15:25Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"angular-deployment-657b4d6b7b-4zbsx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f8c86bfb2353847c081ecb0802fb81c8086f0cc3acfc5b88378b541190357a56\""
Dec 11 15:15:26 minikube cri-dockerd[1196]: time="2023-12-11T15:15:26Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-t2rps_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cd79749d4a16b8007b21dbe980d4f1ca5b57962ccf3df5e4a1416449a5547dbb\""
Dec 11 15:15:26 minikube cri-dockerd[1196]: time="2023-12-11T15:15:26Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-t2rps_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c6144addbf2662cd8db8f868db9f4e352d8db3f0bae58cbb969e58cd162aea8a\""
Dec 11 15:15:27 minikube cri-dockerd[1196]: time="2023-12-11T15:15:27Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-t2rps_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cd79749d4a16b8007b21dbe980d4f1ca5b57962ccf3df5e4a1416449a5547dbb\""
Dec 11 15:15:27 minikube cri-dockerd[1196]: time="2023-12-11T15:15:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7cec90fc85eea019d8092a772865a91b31cbab7be789d7add33129ce1d4f55ea/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 11 15:15:27 minikube cri-dockerd[1196]: time="2023-12-11T15:15:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b4bb4cdc80c50e0f161e4a0f686b5e2798f949bee7ffa3e6d5ff93f396470201/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 11 15:15:27 minikube cri-dockerd[1196]: time="2023-12-11T15:15:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7f5fab657eca421c7ac506105d8d3c194dbe7cdd949f143696a5c0c2e3911d20/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 11 15:15:27 minikube cri-dockerd[1196]: time="2023-12-11T15:15:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/194be3d5aab6a63c7e9efb41d4ea5a8e0dc01ee7d9e8b61dc7fdcfb847c74de2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 11 15:15:28 minikube cri-dockerd[1196]: time="2023-12-11T15:15:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"angular-deployment-657b4d6b7b-4zbsx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"608e92bd9f23f1db5031aecc4fb870090671e288fcabeb7eb509cf2c6bc3a6bb\""
Dec 11 15:15:28 minikube cri-dockerd[1196]: time="2023-12-11T15:15:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"angular-deployment-657b4d6b7b-4zbsx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f8c86bfb2353847c081ecb0802fb81c8086f0cc3acfc5b88378b541190357a56\""
Dec 11 15:15:28 minikube cri-dockerd[1196]: time="2023-12-11T15:15:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-8694d4445c-6ttp9_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f4c305472f21ee1d58349f31e8ef5c9d7bbc9212c7fb4a33165d560d2e7e7e75\""
Dec 11 15:15:28 minikube cri-dockerd[1196]: time="2023-12-11T15:15:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-lsrvg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"05892b1e090c9f4b47efc9e634d0749cabfc747c64e7783e31256c57aecc77e7\""
Dec 11 15:15:28 minikube cri-dockerd[1196]: time="2023-12-11T15:15:28Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-lsrvg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6b88956cf33560d55bb08627e9673d16eb13e2d390aa1c5eda3e5d964d188379\""
Dec 11 15:15:29 minikube cri-dockerd[1196]: time="2023-12-11T15:15:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"angular-deployment-657b4d6b7b-4zbsx_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"608e92bd9f23f1db5031aecc4fb870090671e288fcabeb7eb509cf2c6bc3a6bb\""
Dec 11 15:15:29 minikube cri-dockerd[1196]: time="2023-12-11T15:15:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-lsrvg_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"05892b1e090c9f4b47efc9e634d0749cabfc747c64e7783e31256c57aecc77e7\""
Dec 11 15:15:29 minikube cri-dockerd[1196]: time="2023-12-11T15:15:29Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-7fd5cb4ddc-t2rps_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"cd79749d4a16b8007b21dbe980d4f1ca5b57962ccf3df5e4a1416449a5547dbb\""
Dec 11 15:15:59 minikube cri-dockerd[1196]: time="2023-12-11T15:15:59Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 11 15:16:04 minikube cri-dockerd[1196]: time="2023-12-11T15:16:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6c2565361b5b90bc5eb5f170140a3c0060757c7665f23f4dee0b1ab28c26558d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 11 15:16:06 minikube dockerd[948]: time="2023-12-11T15:16:06.603717094Z" level=info msg="ignoring event" container=7decaa3877834a032b1a7badac0449f3741f536e3ce09fcd85dffeaaa004b226 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 11 15:16:08 minikube cri-dockerd[1196]: time="2023-12-11T15:16:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/323ffacc3517e3473611b47693f881fc10c45a0a2c5737d339116f856bb0a085/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 11 15:16:09 minikube cri-dockerd[1196]: time="2023-12-11T15:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/55d672c11b688660105c800c4925271e7477de6304b48058858ead0d0579eef0/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 11 15:16:09 minikube cri-dockerd[1196]: time="2023-12-11T15:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/06c8fd5df6ac05dff8db087f7154f9d332f8b0118165d4ce698a0f045144a7f5/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 11 15:16:09 minikube cri-dockerd[1196]: time="2023-12-11T15:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cad0d4ac6dc0e9d5d46bca562cc5636809228f5175e7d92505e6af2f35794bdb/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 11 15:16:09 minikube cri-dockerd[1196]: time="2023-12-11T15:16:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2e66f7dbdcffd1030278fd81987d2d2195f021032f7117e4b9f152b0b2631615/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 11 15:16:13 minikube dockerd[948]: time="2023-12-11T15:16:13.253802929Z" level=info msg="ignoring event" container=7a5f48a8805fe6c6694c7678afad477f8cfd38281ecad65014da1c7916e7e8bf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 11 15:16:15 minikube cri-dockerd[1196]: time="2023-12-11T15:16:15Z" level=error msg="Error response from daemon: No such container: 72b7c5030eb8a9b2ed735509d8a87ae318e6b4b317c1869a46a93cc9b3a855fc Failed to get stats from container 72b7c5030eb8a9b2ed735509d8a87ae318e6b4b317c1869a46a93cc9b3a855fc"
Dec 11 15:16:28 minikube cri-dockerd[1196]: time="2023-12-11T15:16:28Z" level=info msg="Pulling image naima68/angular-project:latest: d7fb62c2e1cc: Download complete "
Dec 11 15:16:38 minikube cri-dockerd[1196]: time="2023-12-11T15:16:38Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [=======>                                           ]  37.18MB/242.5MB"
Dec 11 15:16:48 minikube cri-dockerd[1196]: time="2023-12-11T15:16:48Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [=============>                                     ]  63.56MB/242.5MB"
Dec 11 15:16:58 minikube cri-dockerd[1196]: time="2023-12-11T15:16:58Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [==============>                                    ]  70.03MB/242.5MB"
Dec 11 15:17:08 minikube cri-dockerd[1196]: time="2023-12-11T15:17:08Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [==============>                                    ]  70.57MB/242.5MB"
Dec 11 15:17:18 minikube cri-dockerd[1196]: time="2023-12-11T15:17:18Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [===================>                               ]  95.36MB/242.5MB"
Dec 11 15:17:28 minikube cri-dockerd[1196]: time="2023-12-11T15:17:28Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [===========================>                       ]  133.5MB/242.5MB"
Dec 11 15:17:38 minikube cri-dockerd[1196]: time="2023-12-11T15:17:38Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [===============================>                   ]  150.8MB/242.5MB"
Dec 11 15:17:48 minikube cri-dockerd[1196]: time="2023-12-11T15:17:48Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [==================================>                ]  169.7MB/242.5MB"
Dec 11 15:17:58 minikube cri-dockerd[1196]: time="2023-12-11T15:17:58Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [=====================================>             ]  183.2MB/242.5MB"
Dec 11 15:18:08 minikube cri-dockerd[1196]: time="2023-12-11T15:18:08Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Downloading [============================================>      ]  216.7MB/242.5MB"
Dec 11 15:18:18 minikube cri-dockerd[1196]: time="2023-12-11T15:18:18Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Extracting [========================>                          ]  118.7MB/242.5MB"
Dec 11 15:18:28 minikube cri-dockerd[1196]: time="2023-12-11T15:18:28Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Extracting [=========================================>         ]    200MB/242.5MB"
Dec 11 15:18:38 minikube cri-dockerd[1196]: time="2023-12-11T15:18:38Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Extracting [============================================>      ]  217.8MB/242.5MB"
Dec 11 15:18:48 minikube cri-dockerd[1196]: time="2023-12-11T15:18:48Z" level=info msg="Pulling image naima68/angular-project:latest: 0857964da555: Extracting [==============================================>    ]  227.8MB/242.5MB"
Dec 11 15:18:57 minikube cri-dockerd[1196]: time="2023-12-11T15:18:57Z" level=info msg="Stop pulling image naima68/angular-project:latest: Status: Downloaded newer image for naima68/angular-project:latest"
Dec 11 16:13:56 minikube dockerd[948]: time="2023-12-11T16:13:56.507349515Z" level=info msg="ignoring event" container=cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 11 16:13:56 minikube dockerd[948]: time="2023-12-11T16:13:56.947320313Z" level=info msg="ignoring event" container=55d672c11b688660105c800c4925271e7477de6304b48058858ead0d0579eef0 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 11 16:32:07 minikube cri-dockerd[1196]: time="2023-12-11T16:32:07Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/572625c6a1995eacefa57a082795ad72215f7faf5db13596866bde9c383406e3/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 11 16:32:17 minikube cri-dockerd[1196]: time="2023-12-11T16:32:17Z" level=info msg="Stop pulling image naima68/angular-project:latest: Status: Downloaded newer image for naima68/angular-project:latest"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                             CREATED              STATE               NAME                        ATTEMPT             POD ID              POD
376efa6b8941c       naima68/angular-project@sha256:ea96243b3b2d5493ea5381248701635c81502bfd476625f7724e47001d5adfb6   About a minute ago   Running             angular                     0                   572625c6a1995       angular-deployment-657b4d6b7b-7dst6
67b4370788b28       6e38f40d628db                                                                                     About an hour ago    Running             storage-provisioner         140                 323ffacc3517e       storage-provisioner
52aad68fb2fad       10baa1ca17068                                                                                     About an hour ago    Running             kube-controller-manager     23                  7cec90fc85eea       kube-controller-manager-minikube
f557e51102aa8       115053965e86b                                                                                     About an hour ago    Running             dashboard-metrics-scraper   17                  cad0d4ac6dc0e       dashboard-metrics-scraper-7fd5cb4ddc-t2rps
aa1a64d067ab0       ead0a4a53df89                                                                                     About an hour ago    Running             coredns                     20                  06c8fd5df6ac0       coredns-5dd5756b68-lsrvg
ebba4b4f43e11       07655ddf2eebe                                                                                     About an hour ago    Running             kubernetes-dashboard        26                  2e66f7dbdcffd       kubernetes-dashboard-8694d4445c-6ttp9
7a5f48a8805fe       6e38f40d628db                                                                                     About an hour ago    Exited              storage-provisioner         139                 323ffacc3517e       storage-provisioner
eb4fe6aa0acfc       bfc896cf80fba                                                                                     About an hour ago    Running             kube-proxy                  20                  6c2565361b5b9       kube-proxy-vvzfd
297eb391f39bd       73deb9a3f7025                                                                                     About an hour ago    Running             etcd                        20                  b4bb4cdc80c50       etcd-minikube
c6deb8439666a       6d1b4fd1b182d                                                                                     About an hour ago    Running             kube-scheduler              22                  194be3d5aab6a       kube-scheduler-minikube
4ae2644cbfca1       5374347291230                                                                                     About an hour ago    Running             kube-apiserver              22                  7f5fab657eca4       kube-apiserver-minikube
7decaa3877834       10baa1ca17068                                                                                     About an hour ago    Exited              kube-controller-manager     22                  7cec90fc85eea       kube-controller-manager-minikube
4662a5e8120c6       07655ddf2eebe                                                                                     2 weeks ago          Exited              kubernetes-dashboard        25                  f4c305472f21e       kubernetes-dashboard-8694d4445c-6ttp9
0276bee9cbe65       115053965e86b                                                                                     2 weeks ago          Exited              dashboard-metrics-scraper   16                  cd79749d4a16b       dashboard-metrics-scraper-7fd5cb4ddc-t2rps
9894f2cb3ae28       5374347291230                                                                                     2 weeks ago          Exited              kube-apiserver              21                  8ac78f467cb60       kube-apiserver-minikube
2dc6015afaa7f       ead0a4a53df89                                                                                     2 weeks ago          Exited              coredns                     19                  05892b1e090c9       coredns-5dd5756b68-lsrvg
669cf8ca44a07       bfc896cf80fba                                                                                     2 weeks ago          Exited              kube-proxy                  19                  663d93960dae7       kube-proxy-vvzfd
461f2ebcf0b47       73deb9a3f7025                                                                                     2 weeks ago          Exited              etcd                        19                  c092c5e4a5728       etcd-minikube
0fe1e69c866a6       6d1b4fd1b182d                                                                                     2 weeks ago          Exited              kube-scheduler              21                  fc5d7303e321a       kube-scheduler-minikube

* 
* ==> coredns [2dc6015afaa7] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:33879 - 48980 "HINFO IN 8702556206819500492.3908399939497179188. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.927967806s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: connection refused
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.519705265s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": dial tcp :8080: i/o timeout (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.82703162s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.304203349s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.475949044s
[WARNING] plugin/health: Local health request to "http://:8080/health" failed: Get "http://:8080/health": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.448580281s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 2.689343258s
[WARNING] plugin/health: Local health request to "http://:8080/health" took more than 1s: 1.083262212s

* 
* ==> coredns [aa1a64d067ab] <==
* .:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:43200 - 37203 "HINFO IN 251231181033974702.7884413424149971754. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.29856184s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_11_12T19_46_55_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 12 Nov 2023 16:46:50 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 11 Dec 2023 16:33:19 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 11 Dec 2023 16:32:34 +0000   Sat, 02 Dec 2023 07:19:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 11 Dec 2023 16:32:34 +0000   Sat, 02 Dec 2023 07:19:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 11 Dec 2023 16:32:34 +0000   Sat, 02 Dec 2023 07:19:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 11 Dec 2023 16:32:34 +0000   Mon, 04 Dec 2023 12:47:55 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8065808Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8065808Ki
  pods:               110
System Info:
  Machine ID:                 18229723c51e44b6a6870dd875c04059
  System UUID:                18229723c51e44b6a6870dd875c04059
  Boot ID:                    3758045b-455a-4bea-b67a-a2e9b71b652f
  Kernel Version:             5.15.90.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     angular-deployment-657b4d6b7b-7dst6           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         79s
  kube-system                 coredns-5dd5756b68-lsrvg                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     28d
  kube-system                 etcd-minikube                                 100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         28d
  kube-system                 kube-apiserver-minikube                       250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 kube-controller-manager-minikube              200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 kube-proxy-vvzfd                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 kube-scheduler-minikube                       100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kubernetes-dashboard        dashboard-metrics-scraper-7fd5cb4ddc-t2rps    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
  kubernetes-dashboard        kubernetes-dashboard-8694d4445c-6ttp9         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         28d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (18%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (2%!)(MISSING)  170Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Dec11 09:52] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3
[  +0.031139] PCI: Fatal: No config space access function found
[  +0.059074] PCI: System does not support PCI
[  +0.053365] kvm: no hardware support
[  +0.000005] kvm: no hardware support
[  +3.155861] FS-Cache: Duplicate cookie detected
[  +0.000886] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000706] FS-Cache: O-cookie d=000000005adf2efd{9P.session} n=000000009354f0f6
[  +0.000870] FS-Cache: O-key=[10] '34323934393337363237'
[  +0.000459] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000552] FS-Cache: N-cookie d=000000005adf2efd{9P.session} n=00000000b69bc0ad
[  +0.000702] FS-Cache: N-key=[10] '34323934393337363237'
[  +1.668090] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000007]  failed 2
[  +0.029108] WSL (1) WARNING: /usr/share/zoneinfo/Africa/Addis_Ababa not found. Is the tzdata package installed?
[  +1.412794] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000914] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000728] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001012] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.636135] WSL (2) ERROR: UtilCreateProcessAndWait:662: /bin/mount failed with 2
[  +0.012950] WSL (1) ERROR: UtilCreateProcessAndWait:684: /bin/mount failed with status 0xff00

[  +0.004614] WSL (1) ERROR: ConfigMountFsTab:2483: Processing fstab with mount -a failed.
[  +0.064653] WSL (1) ERROR: ConfigApplyWindowsLibPath:2431: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.108838] WSL (1) WARNING: /usr/share/zoneinfo/Africa/Addis_Ababa not found. Is the tzdata package installed?
[  +0.207299] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.027027] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.015718] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.015689] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.636559] netlink: 'init': attribute type 4 has an invalid length.
[  +1.085713] kmem.limit_in_bytes is deprecated and will be removed. Please report your usecase to linux-mm@kvack.org if you depend on this functionality.
[Dec11 10:09] hrtimer: interrupt took 15941634 ns
[Dec11 15:16] overlayfs: lowerdir is in-use as upperdir/workdir of another mount, accessing files from both mounts will result in undefined behavior.

* 
* ==> etcd [297eb391f39b] <==
* {"level":"info","ts":"2023-12-11T16:19:25.174442Z","caller":"traceutil/trace.go:171","msg":"trace[212675502] transaction","detail":"{read_only:false; response_revision:236820; number_of_response:1; }","duration":"108.411984ms","start":"2023-12-11T16:19:25.066003Z","end":"2023-12-11T16:19:25.174415Z","steps":["trace[212675502] 'process raft request'  (duration: 108.252974ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:20:50.479413Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":236647}
{"level":"info","ts":"2023-12-11T16:20:50.480602Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":236647,"took":"811.792s","hash":2831190403}
{"level":"info","ts":"2023-12-11T16:20:50.480672Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2831190403,"revision":236647,"compact-revision":236392}
{"level":"info","ts":"2023-12-11T16:21:01.207735Z","caller":"traceutil/trace.go:171","msg":"trace[2002140394] transaction","detail":"{read_only:false; response_revision:236899; number_of_response:1; }","duration":"149.525971ms","start":"2023-12-11T16:21:01.058188Z","end":"2023-12-11T16:21:01.207714Z","steps":["trace[2002140394] 'process raft request'  (duration: 149.427671ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-11T16:21:02.752025Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.952703ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-11T16:21:02.752097Z","caller":"traceutil/trace.go:171","msg":"trace[947234963] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:236900; }","duration":"128.038903ms","start":"2023-12-11T16:21:02.624043Z","end":"2023-12-11T16:21:02.752082Z","steps":["trace[947234963] 'range keys from in-memory index tree'  (duration: 127.809103ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:21:03.327449Z","caller":"traceutil/trace.go:171","msg":"trace[420736227] transaction","detail":"{read_only:false; response_revision:236901; number_of_response:1; }","duration":"111.11885ms","start":"2023-12-11T16:21:03.216308Z","end":"2023-12-11T16:21:03.327427Z","steps":["trace[420736227] 'process raft request'  (duration: 57.448281ms)","trace[420736227] 'compare'  (duration: 53.530069ms)"],"step_count":2}
{"level":"info","ts":"2023-12-11T16:22:26.481938Z","caller":"traceutil/trace.go:171","msg":"trace[1620472488] transaction","detail":"{read_only:false; response_revision:236966; number_of_response:1; }","duration":"110.207233ms","start":"2023-12-11T16:22:26.3717Z","end":"2023-12-11T16:22:26.481907Z","steps":["trace[1620472488] 'process raft request'  (duration: 110.020533ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:22:28.743766Z","caller":"traceutil/trace.go:171","msg":"trace[924842167] transaction","detail":"{read_only:false; response_revision:236969; number_of_response:1; }","duration":"168.949966ms","start":"2023-12-11T16:22:28.574796Z","end":"2023-12-11T16:22:28.743746Z","steps":["trace[924842167] 'process raft request'  (duration: 74.746922ms)","trace[924842167] 'compare'  (duration: 94.125253ms)"],"step_count":2}
{"level":"info","ts":"2023-12-11T16:22:33.161357Z","caller":"traceutil/trace.go:171","msg":"trace[1263024481] transaction","detail":"{read_only:false; response_revision:236971; number_of_response:1; }","duration":"292.669861ms","start":"2023-12-11T16:22:32.868658Z","end":"2023-12-11T16:22:33.161328Z","steps":["trace[1263024481] 'process raft request'  (duration: 292.511777ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-11T16:22:33.591381Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"329.945782ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025743303957718 > lease_revoke:<id:70cc8c5972dcdc4f>","response":"size:30"}
{"level":"info","ts":"2023-12-11T16:22:37.342014Z","caller":"traceutil/trace.go:171","msg":"trace[1674101406] transaction","detail":"{read_only:false; response_revision:236974; number_of_response:1; }","duration":"118.987ms","start":"2023-12-11T16:22:37.223005Z","end":"2023-12-11T16:22:37.341992Z","steps":["trace[1674101406] 'process raft request'  (duration: 118.857528ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:22:44.398196Z","caller":"traceutil/trace.go:171","msg":"trace[1492621104] transaction","detail":"{read_only:false; response_revision:236980; number_of_response:1; }","duration":"115.013645ms","start":"2023-12-11T16:22:44.283161Z","end":"2023-12-11T16:22:44.398174Z","steps":["trace[1492621104] 'process raft request'  (duration: 114.892571ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:23:38.091605Z","caller":"traceutil/trace.go:171","msg":"trace[312597692] transaction","detail":"{read_only:false; response_revision:237022; number_of_response:1; }","duration":"111.160292ms","start":"2023-12-11T16:23:37.980419Z","end":"2023-12-11T16:23:38.09158Z","steps":["trace[312597692] 'process raft request'  (duration: 102.964329ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:23:42.780529Z","caller":"traceutil/trace.go:171","msg":"trace[733108276] transaction","detail":"{read_only:false; response_revision:237026; number_of_response:1; }","duration":"118.638756ms","start":"2023-12-11T16:23:42.661863Z","end":"2023-12-11T16:23:42.780501Z","steps":["trace[733108276] 'process raft request'  (duration: 118.423958ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:23:47.018432Z","caller":"traceutil/trace.go:171","msg":"trace[1316269948] transaction","detail":"{read_only:false; response_revision:237029; number_of_response:1; }","duration":"161.247336ms","start":"2023-12-11T16:23:46.85715Z","end":"2023-12-11T16:23:47.018397Z","steps":["trace[1316269948] 'process raft request'  (duration: 161.053339ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:24:01.382404Z","caller":"traceutil/trace.go:171","msg":"trace[2128384337] transaction","detail":"{read_only:false; response_revision:237040; number_of_response:1; }","duration":"166.290069ms","start":"2023-12-11T16:24:01.216093Z","end":"2023-12-11T16:24:01.382383Z","steps":["trace[2128384337] 'process raft request'  (duration: 165.975571ms)"],"step_count":1}
WARNING: 2023/12/11 16:24:01 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
WARNING: 2023/12/11 16:24:01 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
WARNING: 2023/12/11 16:24:01 [core] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
{"level":"warn","ts":"2023-12-11T16:24:01.798161Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"115.341885ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-11T16:24:01.798306Z","caller":"traceutil/trace.go:171","msg":"trace[1435812832] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:237041; }","duration":"115.490584ms","start":"2023-12-11T16:24:01.682794Z","end":"2023-12-11T16:24:01.798285Z","steps":["trace[1435812832] 'range keys from in-memory index tree'  (duration: 115.246385ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:24:05.756771Z","caller":"traceutil/trace.go:171","msg":"trace[1260916359] transaction","detail":"{read_only:false; response_revision:237043; number_of_response:1; }","duration":"106.900434ms","start":"2023-12-11T16:24:05.649851Z","end":"2023-12-11T16:24:05.756752Z","steps":["trace[1260916359] 'process raft request'  (duration: 106.562436ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:25:50.519359Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":236889}
{"level":"info","ts":"2023-12-11T16:25:50.520866Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":236889,"took":"1.215153ms","hash":616424731}
{"level":"info","ts":"2023-12-11T16:25:50.520955Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":616424731,"revision":236889,"compact-revision":236647}
{"level":"warn","ts":"2023-12-11T16:27:13.829498Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"172.204601ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-11T16:27:13.829633Z","caller":"traceutil/trace.go:171","msg":"trace[283373474] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:237193; }","duration":"172.3553ms","start":"2023-12-11T16:27:13.657252Z","end":"2023-12-11T16:27:13.829607Z","steps":["trace[283373474] 'range keys from in-memory index tree'  (duration: 172.043501ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:27:14.108708Z","caller":"traceutil/trace.go:171","msg":"trace[306706793] transaction","detail":"{read_only:false; response_revision:237194; number_of_response:1; }","duration":"144.313897ms","start":"2023-12-11T16:27:13.964365Z","end":"2023-12-11T16:27:14.108679Z","steps":["trace[306706793] 'process raft request'  (duration: 144.063798ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:27:31.262133Z","caller":"traceutil/trace.go:171","msg":"trace[319697895] transaction","detail":"{read_only:false; response_revision:237208; number_of_response:1; }","duration":"114.00307ms","start":"2023-12-11T16:27:31.148108Z","end":"2023-12-11T16:27:31.262111Z","steps":["trace[319697895] 'process raft request'  (duration: 113.812968ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:27:35.341471Z","caller":"traceutil/trace.go:171","msg":"trace[1494632055] transaction","detail":"{read_only:false; response_revision:237211; number_of_response:1; }","duration":"187.118879ms","start":"2023-12-11T16:27:35.154327Z","end":"2023-12-11T16:27:35.341446Z","steps":["trace[1494632055] 'process raft request'  (duration: 186.929288ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-11T16:27:41.120115Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"144.443776ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllers/\" range_end:\"/registry/controllers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-11T16:27:41.120319Z","caller":"traceutil/trace.go:171","msg":"trace[91558654] range","detail":"{range_begin:/registry/controllers/; range_end:/registry/controllers0; response_count:0; response_revision:237215; }","duration":"144.58047ms","start":"2023-12-11T16:27:40.975636Z","end":"2023-12-11T16:27:41.120216Z","steps":["trace[91558654] 'count revisions from in-memory index tree'  (duration: 144.333482ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:27:49.242146Z","caller":"traceutil/trace.go:171","msg":"trace[1605032446] transaction","detail":"{read_only:false; response_revision:237222; number_of_response:1; }","duration":"142.171739ms","start":"2023-12-11T16:27:49.099951Z","end":"2023-12-11T16:27:49.242123Z","steps":["trace[1605032446] 'process raft request'  (duration: 142.014391ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:27:51.889931Z","caller":"traceutil/trace.go:171","msg":"trace[118131558] transaction","detail":"{read_only:false; response_revision:237224; number_of_response:1; }","duration":"125.785038ms","start":"2023-12-11T16:27:51.764125Z","end":"2023-12-11T16:27:51.88991Z","steps":["trace[118131558] 'process raft request'  (duration: 125.636641ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:27:59.664325Z","caller":"traceutil/trace.go:171","msg":"trace[120113049] transaction","detail":"{read_only:false; response_revision:237230; number_of_response:1; }","duration":"142.040239ms","start":"2023-12-11T16:27:59.522265Z","end":"2023-12-11T16:27:59.664305Z","steps":["trace[120113049] 'process raft request'  (duration: 141.903728ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:28:05.908156Z","caller":"traceutil/trace.go:171","msg":"trace[2109638464] transaction","detail":"{read_only:false; response_revision:237234; number_of_response:1; }","duration":"101.929859ms","start":"2023-12-11T16:28:05.806198Z","end":"2023-12-11T16:28:05.908128Z","steps":["trace[2109638464] 'process raft request'  (duration: 101.695973ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:28:14.317283Z","caller":"traceutil/trace.go:171","msg":"trace[1312466686] transaction","detail":"{read_only:false; response_revision:237241; number_of_response:1; }","duration":"122.111437ms","start":"2023-12-11T16:28:14.195146Z","end":"2023-12-11T16:28:14.317258Z","steps":["trace[1312466686] 'process raft request'  (duration: 121.940846ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:28:16.267915Z","caller":"traceutil/trace.go:171","msg":"trace[1743774805] transaction","detail":"{read_only:false; response_revision:237242; number_of_response:1; }","duration":"125.560008ms","start":"2023-12-11T16:28:16.142327Z","end":"2023-12-11T16:28:16.267887Z","steps":["trace[1743774805] 'process raft request'  (duration: 125.333985ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-11T16:28:16.594254Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"171.649481ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1113"}
{"level":"info","ts":"2023-12-11T16:28:16.594364Z","caller":"traceutil/trace.go:171","msg":"trace[1397095670] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:237242; }","duration":"171.776326ms","start":"2023-12-11T16:28:16.422568Z","end":"2023-12-11T16:28:16.594344Z","steps":["trace[1397095670] 'range keys from in-memory index tree'  (duration: 171.514747ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:28:28.128486Z","caller":"traceutil/trace.go:171","msg":"trace[686060009] transaction","detail":"{read_only:false; response_revision:237252; number_of_response:1; }","duration":"103.792598ms","start":"2023-12-11T16:28:28.02467Z","end":"2023-12-11T16:28:28.128463Z","steps":["trace[686060009] 'process raft request'  (duration: 102.067952ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:28:33.208642Z","caller":"traceutil/trace.go:171","msg":"trace[344944287] transaction","detail":"{read_only:false; response_revision:237256; number_of_response:1; }","duration":"103.884293ms","start":"2023-12-11T16:28:33.104736Z","end":"2023-12-11T16:28:33.20862Z","steps":["trace[344944287] 'process raft request'  (duration: 103.699308ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:29:24.194363Z","caller":"traceutil/trace.go:171","msg":"trace[739920031] transaction","detail":"{read_only:false; response_revision:237295; number_of_response:1; }","duration":"217.789083ms","start":"2023-12-11T16:29:23.976551Z","end":"2023-12-11T16:29:24.19434Z","steps":["trace[739920031] 'process raft request'  (duration: 217.474881ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:29:50.638401Z","caller":"traceutil/trace.go:171","msg":"trace[1169961156] transaction","detail":"{read_only:false; response_revision:237317; number_of_response:1; }","duration":"107.396223ms","start":"2023-12-11T16:29:50.530985Z","end":"2023-12-11T16:29:50.638381Z","steps":["trace[1169961156] 'process raft request'  (duration: 107.27192ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:30:09.339287Z","caller":"traceutil/trace.go:171","msg":"trace[1629742979] transaction","detail":"{read_only:false; response_revision:237332; number_of_response:1; }","duration":"129.089813ms","start":"2023-12-11T16:30:09.210049Z","end":"2023-12-11T16:30:09.339138Z","steps":["trace[1629742979] 'process raft request'  (duration: 128.91301ms)"],"step_count":1}
{"level":"info","ts":"2023-12-11T16:30:50.834004Z","caller":"traceutil/trace.go:171","msg":"trace[1769902068] transaction","detail":"{read_only:false; response_revision:237365; number_of_response:1; }","duration":"232.912532ms","start":"2023-12-11T16:30:50.601063Z","end":"2023-12-11T16:30:50.833975Z","steps":["trace[1769902068] 'process raft request'  (duration: 177.233091ms)","trace[1769902068] 'compare'  (duration: 55.520284ms)"],"step_count":2}
{"level":"info","ts":"2023-12-11T16:30:50.984947Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":237127}
{"level":"info","ts":"2023-12-11T16:30:50.987282Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":237127,"took":"1.966608ms","hash":1747423648}
{"level":"info","ts":"2023-12-11T16:30:50.987542Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1747423648,"revision":237127,"compact-revision":236889}
{"level":"warn","ts":"2023-12-11T16:30:51.182712Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"126.397194ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025743303960328 username:\"kube-apiserver-etcd-client\" auth_revision:1 > compaction:<revision:237127 > ","response":"size:6"}
{"level":"info","ts":"2023-12-11T16:30:51.182951Z","caller":"traceutil/trace.go:171","msg":"trace[1389955267] linearizableReadLoop","detail":"{readStateIndex:301365; appliedIndex:301364; }","duration":"318.912786ms","start":"2023-12-11T16:30:50.864013Z","end":"2023-12-11T16:30:51.182926Z","steps":["trace[1389955267] 'read index received'  (duration: 27.409s)","trace[1389955267] 'applied index is now lower than readState.Index'  (duration: 318.882176ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-11T16:30:51.183097Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"319.077246ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-12-11T16:30:51.183161Z","caller":"traceutil/trace.go:171","msg":"trace[162498682] range","detail":"{range_begin:/registry/prioritylevelconfigurations/; range_end:/registry/prioritylevelconfigurations0; response_count:0; response_revision:237365; }","duration":"319.156374ms","start":"2023-12-11T16:30:50.863983Z","end":"2023-12-11T16:30:51.18314Z","steps":["trace[162498682] 'agreement among raft nodes before linearized reading'  (duration: 319.027928ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-11T16:30:51.183246Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-11T16:30:50.863969Z","time spent":"319.248407ms","remote":"127.0.0.1:56434","response type":"/etcdserverpb.KV/Range","request count":0,"request size":82,"response count":8,"response size":32,"request content":"key:\"/registry/prioritylevelconfigurations/\" range_end:\"/registry/prioritylevelconfigurations0\" count_only:true "}
{"level":"info","ts":"2023-12-11T16:30:51.183465Z","caller":"traceutil/trace.go:171","msg":"trace[727293657] compact","detail":"{revision:237127; response_revision:237365; }","duration":"534.573409ms","start":"2023-12-11T16:30:50.648407Z","end":"2023-12-11T16:30:51.182981Z","steps":["trace[727293657] 'process raft request'  (duration: 210.031996ms)","trace[727293657] 'check and update compact revision'  (duration: 126.264147ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-11T16:30:51.183551Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-11T16:30:50.648395Z","time spent":"535.152217ms","remote":"127.0.0.1:41320","response type":"/etcdserverpb.KV/Compact","request count":-1,"request size":-1,"response count":-1,"response size":-1,"request content":""}
{"level":"warn","ts":"2023-12-11T16:31:51.823994Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"122.727979ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-11T16:31:51.824087Z","caller":"traceutil/trace.go:171","msg":"trace[1244803375] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:237414; }","duration":"122.838202ms","start":"2023-12-11T16:31:51.701232Z","end":"2023-12-11T16:31:51.82407Z","steps":["trace[1244803375] 'range keys from in-memory index tree'  (duration: 122.622458ms)"],"step_count":1}

* 
* ==> etcd [461f2ebcf0b4] <==
* {"level":"info","ts":"2023-12-04T12:47:45.142794Z","caller":"traceutil/trace.go:171","msg":"trace[2080387925] range","detail":"{range_begin:/registry/ingressclasses/; range_end:/registry/ingressclasses0; response_count:0; response_revision:232870; }","duration":"3.088585409s","start":"2023-12-04T09:00:01.046982Z","end":"2023-12-04T12:47:45.142744Z","steps":["trace[2080387925] 'agreement among raft nodes before linearized reading'  (duration: 3.088388208s)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:47:45.142899Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T09:00:01.046973Z","time spent":"3.08869101s","remote":"127.0.0.1:37450","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":0,"response size":30,"request content":"key:\"/registry/ingressclasses/\" range_end:\"/registry/ingressclasses0\" count_only:true "}
{"level":"info","ts":"2023-12-04T12:47:45.136537Z","caller":"traceutil/trace.go:171","msg":"trace[354770708] range","detail":"{range_begin:/registry/mutatingwebhookconfigurations/; range_end:/registry/mutatingwebhookconfigurations0; response_count:0; response_revision:232870; }","duration":"3.082178489s","start":"2023-12-04T09:00:01.047169Z","end":"2023-12-04T12:47:45.136524Z","steps":["trace[354770708] 'agreement among raft nodes before linearized reading'  (duration: 3.082041289s)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:47:45.153122Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T09:00:01.047166Z","time spent":"3.098694442s","remote":"127.0.0.1:49664","response type":"/etcdserverpb.KV/Range","request count":0,"request size":86,"response count":0,"response size":30,"request content":"key:\"/registry/mutatingwebhookconfigurations/\" range_end:\"/registry/mutatingwebhookconfigurations0\" count_only:true "}
{"level":"warn","ts":"2023-12-04T12:47:45.153374Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.179736499s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-12-04T12:47:45.153434Z","caller":"traceutil/trace.go:171","msg":"trace[1413429730] range","detail":"{range_begin:/registry/configmaps/; range_end:/registry/configmaps0; response_count:0; response_revision:232870; }","duration":"3.179816499s","start":"2023-12-04T09:00:00.966422Z","end":"2023-12-04T12:47:45.153415Z","steps":["trace[1413429730] 'agreement among raft nodes before linearized reading'  (duration: 3.179663398s)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:47:45.153486Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T09:00:00.966414Z","time spent":"3.179880399s","remote":"127.0.0.1:60062","response type":"/etcdserverpb.KV/Range","request count":0,"request size":48,"response count":13,"response size":32,"request content":"key:\"/registry/configmaps/\" range_end:\"/registry/configmaps0\" count_only:true "}
{"level":"warn","ts":"2023-12-04T12:47:45.191266Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.136945063s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-12-04T12:47:45.191767Z","caller":"traceutil/trace.go:171","msg":"trace[77264024] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:232870; }","duration":"3.137454865s","start":"2023-12-04T09:00:01.047114Z","end":"2023-12-04T12:47:45.191745Z","steps":["trace[77264024] 'agreement among raft nodes before linearized reading'  (duration: 3.136898563s)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:47:45.191834Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T09:00:01.047109Z","time spent":"3.137524065s","remote":"127.0.0.1:49604","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":4,"response size":32,"request content":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true "}
{"level":"warn","ts":"2023-12-04T12:47:45.192063Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.137733566s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-04T12:47:45.192104Z","caller":"traceutil/trace.go:171","msg":"trace[846791285] range","detail":"{range_begin:/registry/horizontalpodautoscalers/; range_end:/registry/horizontalpodautoscalers0; response_count:0; response_revision:232870; }","duration":"3.137775466s","start":"2023-12-04T09:00:01.047139Z","end":"2023-12-04T12:47:45.192091Z","steps":["trace[846791285] 'agreement among raft nodes before linearized reading'  (duration: 3.137703866s)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:47:45.192152Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T09:00:01.047136Z","time spent":"3.137824466s","remote":"127.0.0.1:59926","response type":"/etcdserverpb.KV/Range","request count":0,"request size":76,"response count":0,"response size":30,"request content":"key:\"/registry/horizontalpodautoscalers/\" range_end:\"/registry/horizontalpodautoscalers0\" count_only:true "}
{"level":"warn","ts":"2023-12-04T12:47:45.203209Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"3.148957301s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-12-04T12:47:45.203265Z","caller":"traceutil/trace.go:171","msg":"trace[1234098595] range","detail":"{range_begin:/registry/flowschemas/; range_end:/registry/flowschemas0; response_count:0; response_revision:232870; }","duration":"3.149028901s","start":"2023-12-04T09:00:01.047047Z","end":"2023-12-04T12:47:45.203253Z","steps":["trace[1234098595] 'agreement among raft nodes before linearized reading'  (duration: 3.148919201s)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:47:45.203304Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T09:00:01.047042Z","time spent":"3.149075201s","remote":"127.0.0.1:33476","response type":"/etcdserverpb.KV/Range","request count":0,"request size":50,"response count":13,"response size":32,"request content":"key:\"/registry/flowschemas/\" range_end:\"/registry/flowschemas0\" count_only:true "}
{"level":"warn","ts":"2023-12-04T12:47:45.388822Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.020133ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025344390408567 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:0 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128025344390408560 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2023-12-04T12:47:45.388976Z","caller":"traceutil/trace.go:171","msg":"trace[4962753] transaction","detail":"{read_only:false; response_revision:232871; number_of_response:1; }","duration":"168.364335ms","start":"2023-12-04T12:47:45.220574Z","end":"2023-12-04T12:47:45.388939Z","steps":["trace[4962753] 'process raft request'  (duration: 63.155301ms)","trace[4962753] 'compare'  (duration: 104.838933ms)"],"step_count":2}
{"level":"info","ts":"2023-12-04T12:47:45.389044Z","caller":"traceutil/trace.go:171","msg":"trace[403964095] linearizableReadLoop","detail":"{readStateIndex:295734; appliedIndex:295733; }","duration":"167.941633ms","start":"2023-12-04T12:47:45.221082Z","end":"2023-12-04T12:47:45.389023Z","steps":["trace[403964095] 'read index received'  (duration: 62.571199ms)","trace[403964095] 'applied index is now lower than readState.Index'  (duration: 105.368434ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-04T12:47:45.389221Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"168.149934ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-t2rps\" ","response":"range_response_count:1 size:4021"}
{"level":"info","ts":"2023-12-04T12:47:45.389277Z","caller":"traceutil/trace.go:171","msg":"trace[314990508] range","detail":"{range_begin:/registry/pods/kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-t2rps; range_end:; response_count:1; response_revision:232871; }","duration":"168.213134ms","start":"2023-12-04T12:47:45.221052Z","end":"2023-12-04T12:47:45.389265Z","steps":["trace[314990508] 'agreement among raft nodes before linearized reading'  (duration: 168.068234ms)"],"step_count":1}
{"level":"info","ts":"2023-12-04T12:47:45.389926Z","caller":"traceutil/trace.go:171","msg":"trace[643795529] transaction","detail":"{read_only:false; response_revision:232872; number_of_response:1; }","duration":"152.194983ms","start":"2023-12-04T12:47:45.237713Z","end":"2023-12-04T12:47:45.389908Z","steps":["trace[643795529] 'process raft request'  (duration: 151.24428ms)"],"step_count":1}
{"level":"info","ts":"2023-12-04T12:47:45.642645Z","caller":"traceutil/trace.go:171","msg":"trace[1058018218] transaction","detail":"{read_only:false; response_revision:232873; number_of_response:1; }","duration":"152.765785ms","start":"2023-12-04T12:47:45.489857Z","end":"2023-12-04T12:47:45.642622Z","steps":["trace[1058018218] 'process raft request'  (duration: 152.575284ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:48:45.777585Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"660.905599ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" ","response":"range_response_count:1 size:134"}
{"level":"info","ts":"2023-12-04T12:48:45.778111Z","caller":"traceutil/trace.go:171","msg":"trace[338508993] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:232920; }","duration":"3.759848443s","start":"2023-12-04T12:48:41.968786Z","end":"2023-12-04T12:48:45.778019Z","steps":["trace[338508993] 'agreement among raft nodes before linearized reading'  (duration: 660.626198ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:48:45.778413Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T12:48:41.968716Z","time spent":"3.760210644s","remote":"127.0.0.1:37164","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":158,"request content":"key:\"/registry/masterleases/192.168.49.2\" "}
{"level":"warn","ts":"2023-12-04T12:48:46.74564Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"127.511105ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/events/\" range_end:\"/registry/events0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-12-04T12:48:46.747295Z","caller":"traceutil/trace.go:171","msg":"trace[651589439] range","detail":"{range_begin:/registry/events/; range_end:/registry/events0; response_count:0; response_revision:232921; }","duration":"129.208411ms","start":"2023-12-04T12:48:46.618068Z","end":"2023-12-04T12:48:46.747276Z","steps":["trace[651589439] 'agreement among raft nodes before linearized reading'  (duration: 127.454605ms)"],"step_count":1}
{"level":"info","ts":"2023-12-04T12:48:46.966148Z","caller":"traceutil/trace.go:171","msg":"trace[745388990] linearizableReadLoop","detail":"{readStateIndex:295798; appliedIndex:295797; }","duration":"185.400389ms","start":"2023-12-04T12:48:46.780716Z","end":"2023-12-04T12:48:46.966116Z","steps":["trace[745388990] 'read index received'  (duration: 136.095932ms)","trace[745388990] 'applied index is now lower than readState.Index'  (duration: 49.303257ms)"],"step_count":2}
{"level":"info","ts":"2023-12-04T12:48:46.966313Z","caller":"traceutil/trace.go:171","msg":"trace[1557819481] transaction","detail":"{read_only:false; response_revision:232923; number_of_response:1; }","duration":"202.720544ms","start":"2023-12-04T12:48:46.763578Z","end":"2023-12-04T12:48:46.966298Z","steps":["trace[1557819481] 'process raft request'  (duration: 153.324787ms)","trace[1557819481] 'compare'  (duration: 49.105756ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-04T12:48:46.966636Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"185.98329ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/generic-garbage-collector\" ","response":"range_response_count:1 size:217"}
{"level":"info","ts":"2023-12-04T12:48:46.966677Z","caller":"traceutil/trace.go:171","msg":"trace[1777065870] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/generic-garbage-collector; range_end:; response_count:1; response_revision:232923; }","duration":"186.036391ms","start":"2023-12-04T12:48:46.780629Z","end":"2023-12-04T12:48:46.966665Z","steps":["trace[1777065870] 'agreement among raft nodes before linearized reading'  (duration: 185.91079ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:48:46.966842Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.652036ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-04T12:48:46.96687Z","caller":"traceutil/trace.go:171","msg":"trace[1715168803] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:232923; }","duration":"105.682536ms","start":"2023-12-04T12:48:46.861179Z","end":"2023-12-04T12:48:46.966862Z","steps":["trace[1715168803] 'agreement among raft nodes before linearized reading'  (duration: 105.633036ms)"],"step_count":1}
{"level":"warn","ts":"2023-12-04T12:50:12.503048Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"255.975279ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-12-04T12:50:12.503673Z","caller":"traceutil/trace.go:171","msg":"trace[255883154] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:232989; }","duration":"303.481431ms","start":"2023-12-04T12:50:12.177482Z","end":"2023-12-04T12:50:12.503385Z","steps":["trace[255883154] 'agreement among raft nodes before linearized reading'  (duration: 191.56809ms)","trace[255883154] 'get authentication metadata'  (duration: 47.487545ms)","trace[255883154] 'range keys from in-memory index tree'  (duration: 16.66695ms)"],"step_count":3}
{"level":"warn","ts":"2023-12-04T12:50:12.503992Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T12:50:12.177402Z","time spent":"304.073725ms","remote":"127.0.0.1:33530","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":30,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2023-12-04T12:50:12.365412Z","caller":"traceutil/trace.go:171","msg":"trace[1694400604] linearizableReadLoop","detail":"{readStateIndex:295881; appliedIndex:295881; }","duration":"187.480366ms","start":"2023-12-04T12:50:12.177663Z","end":"2023-12-04T12:50:12.365143Z","steps":["trace[1694400604] 'read index received'  (duration: 187.432166ms)","trace[1694400604] 'applied index is now lower than readState.Index'  (duration: 40.1s)"],"step_count":2}
{"level":"warn","ts":"2023-12-04T12:50:13.567412Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"210.083932ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025344390409147 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:232983 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128025344390409145 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >>","response":"size:18"}
{"level":"info","ts":"2023-12-04T12:50:13.56806Z","caller":"traceutil/trace.go:171","msg":"trace[1102710505] transaction","detail":"{read_only:false; response_revision:232990; number_of_response:1; }","duration":"720.757607ms","start":"2023-12-04T12:50:12.847174Z","end":"2023-12-04T12:50:13.567932Z","steps":["trace[1102710505] 'process raft request'  (duration: 509.661071ms)","trace[1102710505] 'compare'  (duration: 78.671474ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-04T12:50:14.325119Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T12:50:12.847077Z","time spent":"722.397912ms","remote":"127.0.0.1:37164","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:232983 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128025344390409145 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"warn","ts":"2023-12-04T12:50:14.706834Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"254.657887ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128025344390409149 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:232989 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2023-12-04T12:50:14.721416Z","caller":"traceutil/trace.go:171","msg":"trace[863129164] transaction","detail":"{read_only:false; response_revision:232991; number_of_response:1; }","duration":"1.32809853s","start":"2023-12-04T12:50:13.380015Z","end":"2023-12-04T12:50:14.708114Z","steps":["trace[863129164] 'process raft request'  (duration: 1.071418736s)","trace[863129164] 'compare'  (duration: 225.021384ms)","trace[863129164] 'get key's previous created_revision and leaseID' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1091; } (duration: 15.774655ms)"],"step_count":3}
{"level":"warn","ts":"2023-12-04T12:50:14.722417Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T12:50:13.379692Z","time spent":"1.342360081s","remote":"127.0.0.1:33554","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1094,"response count":0,"response size":42,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:232989 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1021 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-12-04T12:50:15.424697Z","caller":"traceutil/trace.go:171","msg":"trace[1115220689] linearizableReadLoop","detail":"{readStateIndex:295884; appliedIndex:295884; }","duration":"461.740709ms","start":"2023-12-04T12:50:14.821349Z","end":"2023-12-04T12:50:15.28309Z","steps":["trace[1115220689] 'read index received'  (duration: 461.445708ms)","trace[1115220689] 'applied index is now lower than readState.Index'  (duration: 285.101s)"],"step_count":2}
{"level":"warn","ts":"2023-12-04T12:50:15.578656Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"757.554538ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/default/kubernetes\" ","response":"range_response_count:1 size:422"}
{"level":"info","ts":"2023-12-04T12:50:15.578735Z","caller":"traceutil/trace.go:171","msg":"trace[1585641250] range","detail":"{range_begin:/registry/services/endpoints/default/kubernetes; range_end:; response_count:1; response_revision:232991; }","duration":"757.65144ms","start":"2023-12-04T12:50:14.821066Z","end":"2023-12-04T12:50:15.578717Z","steps":["trace[1585641250] 'agreement among raft nodes before linearized reading'  (duration: 478.102966ms)","trace[1585641250] 'range keys from in-memory index tree'  (duration: 279.248673ms)"],"step_count":2}
{"level":"warn","ts":"2023-12-04T12:50:15.578782Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-12-04T12:50:14.82099Z","time spent":"757.77884ms","remote":"127.0.0.1:33554","response type":"/etcdserverpb.KV/Range","request count":0,"request size":49,"response count":1,"response size":446,"request content":"key:\"/registry/services/endpoints/default/kubernetes\" "}
{"level":"info","ts":"2023-12-04T12:50:48.113426Z","caller":"traceutil/trace.go:171","msg":"trace[920226750] transaction","detail":"{read_only:false; response_revision:233017; number_of_response:1; }","duration":"101.916714ms","start":"2023-12-04T12:50:48.011486Z","end":"2023-12-04T12:50:48.113403Z","steps":["trace[920226750] 'process raft request'  (duration: 101.782015ms)"],"step_count":1}
{"level":"info","ts":"2023-12-04T12:51:39.674736Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":232840}
{"level":"info","ts":"2023-12-04T12:51:39.676249Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":232840,"took":"1.216476ms","hash":3240215438}
{"level":"info","ts":"2023-12-04T12:51:39.676337Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3240215438,"revision":232840,"compact-revision":232600}
{"level":"info","ts":"2023-12-04T12:56:39.671302Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":233058}
{"level":"info","ts":"2023-12-04T12:56:39.672518Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":233058,"took":"886.998s","hash":3117636653}
{"level":"info","ts":"2023-12-04T12:56:39.672582Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3117636653,"revision":233058,"compact-revision":232840}
{"level":"info","ts":"2023-12-04T12:58:12.249885Z","caller":"traceutil/trace.go:171","msg":"trace[1700545806] transaction","detail":"{read_only:false; response_revision:233373; number_of_response:1; }","duration":"108.612804ms","start":"2023-12-04T12:58:12.141227Z","end":"2023-12-04T12:58:12.24984Z","steps":["trace[1700545806] 'process raft request'  (duration: 92.101666ms)"],"step_count":1}
{"level":"info","ts":"2023-12-04T13:01:39.681454Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":233298}
{"level":"info","ts":"2023-12-04T13:01:39.683439Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":233298,"took":"1.589508ms","hash":1209435582}
{"level":"info","ts":"2023-12-04T13:01:39.683498Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1209435582,"revision":233298,"compact-revision":233058}
{"level":"info","ts":"2023-12-04T13:04:32.485811Z","caller":"traceutil/trace.go:171","msg":"trace[508617629] transaction","detail":"{read_only:false; response_revision:233676; number_of_response:1; }","duration":"160.213366ms","start":"2023-12-04T13:04:32.32557Z","end":"2023-12-04T13:04:32.485784Z","steps":["trace[508617629] 'process raft request'  (duration: 159.965864ms)"],"step_count":1}

* 
* ==> kernel <==
*  16:33:25 up  6:40,  0 users,  load average: 2.12, 3.25, 2.71
Linux minikube 5.15.90.1-microsoft-standard-WSL2 #1 SMP Fri Jan 27 02:56:13 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kube-apiserver [4ae2644cbfca] <==
* Trace[813274818]: ---"Write to database call succeeded" len:148 1287ms (15:15:59.592)
Trace[813274818]: [1.287660743s] [1.287660743s] END
I1211 15:15:59.600873       1 trace.go:236] Trace[712348643]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:33e221b0-afb8-4887-a07f-8c3197639841,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/serviceaccounts/kubernetes-dashboard/token,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (11-Dec-2023 15:15:58.402) (total time: 1198ms):
Trace[712348643]: ---"limitedReadBody succeeded" len:169 853ms (15:15:59.255)
Trace[712348643]: ---"Write to database call succeeded" len:169 344ms (15:15:59.600)
Trace[712348643]: [1.198513708s] [1.198513708s] END
I1211 15:15:59.611802       1 trace.go:236] Trace[1232551317]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:ce0848a1-eb75-40f9-90f8-264cf443ebc9,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kubernetes-dashboard/serviceaccounts/kubernetes-dashboard/token,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (11-Dec-2023 15:15:58.403) (total time: 1208ms):
Trace[1232551317]: ---"limitedReadBody succeeded" len:174 898ms (15:15:59.301)
Trace[1232551317]: ---"Write to database call succeeded" len:174 310ms (15:15:59.611)
Trace[1232551317]: [1.208739794s] [1.208739794s] END
I1211 15:16:02.760024       1 trace.go:236] Trace[1148062576]: "Get" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:f9d2e008-1e1d-49ee-b481-9a09e872ac52,client:192.168.49.2,protocol:HTTP/2.0,resource:pods,scope:resource,url:/api/v1/namespaces/kube-system/pods/storage-provisioner,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (11-Dec-2023 15:16:02.149) (total time: 610ms):
Trace[1148062576]: ---"About to write a response" 610ms (15:16:02.759)
Trace[1148062576]: [610.177135ms] [610.177135ms] END
I1211 15:16:02.760647       1 trace.go:236] Trace[44693670]: "Patch" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:fffa94fc-459b-4ea6-8fdf-6bf078c69972,client:192.168.49.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/default/events/minikube.179fcfdb61bae6c9,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PATCH (11-Dec-2023 15:16:01.927) (total time: 833ms):
Trace[44693670]: ["GuaranteedUpdate etcd3" audit-id:fffa94fc-459b-4ea6-8fdf-6bf078c69972,key:/events/default/minikube.179fcfdb61bae6c9,type:*core.Event,resource:events 833ms (15:16:01.927)
Trace[44693670]:  ---"initial value restored" 80ms (15:16:02.007)
Trace[44693670]:  ---"Txn call completed" 751ms (15:16:02.760)]
Trace[44693670]: ---"Object stored in database" 751ms (15:16:02.760)
Trace[44693670]: [833.531966ms] [833.531966ms] END
I1211 15:16:02.761178       1 trace.go:236] Trace[1845368732]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:e344a9a2-ebc3-43dc-a9fc-3ed2c457a6d6,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterroles,scope:resource,url:/apis/rbac.authorization.k8s.io/v1/clusterroles/system:service-account-issuer-discovery,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (11-Dec-2023 15:16:02.140) (total time: 620ms):
Trace[1845368732]: ---"About to write a response" 620ms (15:16:02.761)
Trace[1845368732]: [620.25041ms] [620.25041ms] END
I1211 15:16:05.882226       1 trace.go:236] Trace[1488261160]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:3cdaad50-fd6b-4f91-aafb-1b013b4e1a35,client:127.0.0.1,protocol:HTTP/2.0,resource:clusterrolebindings,scope:resource,url:/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:controller:pv-protection-controller,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (11-Dec-2023 15:16:05.020) (total time: 861ms):
Trace[1488261160]: ---"About to write a response" 861ms (15:16:05.882)
Trace[1488261160]: [861.887814ms] [861.887814ms] END
E1211 15:16:06.859448       1 controller.go:193] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 344cee46-2786-471b-bba3-c5f93d7d6ac4, UID in object meta: "
I1211 15:16:09.509118       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I1211 15:16:09.569960       1 controller.go:624] quota admission added evaluator for: deployments.apps
I1211 15:16:09.811685       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I1211 15:16:10.006735       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1211 15:16:10.057201       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
E1211 15:16:19.708230       1 storage.go:475] Address {10.244.0.83  0xc0047ce960 0xc002db10a0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [{10.244.0.86}] vs 10.244.0.83 (kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc-t2rps))
E1211 15:16:19.708333       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.83  0xc0047ce960 0xc002db10a0}] [] [{ 8000 TCP <nil>}]}
I1211 15:16:38.890520       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1211 15:16:38.897297       1 controller.go:624] quota admission added evaluator for: endpoints
I1211 15:23:47.133326       1 trace.go:236] Trace[1223249390]: "Update" accept:application/json, */*,audit-id:8e7f8c38-82f4-449a-b7e7-e4437c19e205,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Dec-2023 15:23:45.192) (total time: 1940ms):
Trace[1223249390]: ["GuaranteedUpdate etcd3" audit-id:8e7f8c38-82f4-449a-b7e7-e4437c19e205,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1929ms (15:23:45.204)
Trace[1223249390]:  ---"Txn call completed" 1927ms (15:23:47.133)]
Trace[1223249390]: [1.940904433s] [1.940904433s] END
I1211 15:32:22.190575       1 trace.go:236] Trace[1559932852]: "Update" accept:application/json, */*,audit-id:87741ce1-996a-46e8-8478-d6b35dbc0ab4,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Dec-2023 15:32:21.094) (total time: 1096ms):
Trace[1559932852]: ["GuaranteedUpdate etcd3" audit-id:87741ce1-996a-46e8-8478-d6b35dbc0ab4,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1095ms (15:32:21.094)
Trace[1559932852]:  ---"About to Encode" 154ms (15:32:21.248)
Trace[1559932852]:  ---"Txn call completed" 941ms (15:32:22.190)]
Trace[1559932852]: [1.096276747s] [1.096276747s] END
I1211 15:36:18.204874       1 trace.go:236] Trace[32089370]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (11-Dec-2023 15:36:17.597) (total time: 607ms):
Trace[32089370]: ---"Transaction prepared" 125ms (15:36:17.724)
Trace[32089370]: ---"Txn call completed" 480ms (15:36:18.204)
Trace[32089370]: [607.393341ms] [607.393341ms] END
I1211 16:13:16.780560       1 trace.go:236] Trace[725853435]: "Update" accept:application/json, */*,audit-id:3b2e7de7-2026-43f8-a6a9-7bf3f85e5d1d,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (11-Dec-2023 16:13:16.222) (total time: 558ms):
Trace[725853435]: ["GuaranteedUpdate etcd3" audit-id:3b2e7de7-2026-43f8-a6a9-7bf3f85e5d1d,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 557ms (16:13:16.222)
Trace[725853435]:  ---"Txn call completed" 552ms (16:13:16.780)]
Trace[725853435]: [558.461999ms] [558.461999ms] END
I1211 16:13:57.903400       1 controller.go:624] quota admission added evaluator for: replicasets.apps
E1211 16:24:01.494892       1 storage.go:475] Address {10.244.0.87  0xc004fcbae0 0xc009750850} isn't valid (context canceled)
E1211 16:24:01.495393       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.87  0xc004fcbae0 0xc009750850}] [] [{ 9090 TCP <nil>}]}
E1211 16:24:01.495505       1 storage.go:475] Address {10.244.0.87  0xc004fcb9c0 0xc009750770} isn't valid (context canceled)
E1211 16:24:01.496800       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.87  0xc004fcb9c0 0xc009750770}] [] [{ 9090 TCP <nil>}]}
E1211 16:24:01.518356       1 storage.go:475] Address {10.244.0.87  0xc004fcb890 0xc009750690} isn't valid (context canceled)
E1211 16:24:01.518429       1 storage.go:485] Failed to find a valid address, skipping subset: &{[{10.244.0.87  0xc004fcb890 0xc009750690}] [] [{ 9090 TCP <nil>}]}
E1211 16:24:01.521319       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled

* 
* ==> kube-apiserver [9894f2cb3ae2] <==
* Trace[1905045925]: ---"Transaction prepared" 114ms (07:47:16.558)
Trace[1905045925]: ---"Txn call completed" 271ms (07:47:16.829)
Trace[1905045925]: [532.035307ms] [532.035307ms] END
I1204 07:47:17.697945       1 trace.go:236] Trace[912791003]: "Update" accept:application/json, */*,audit-id:8225569c-5b09-48f2-bd88-f7982bd52da1,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (04-Dec-2023 07:47:17.149) (total time: 547ms):
Trace[912791003]: ---"Conversion done" 175ms (07:47:17.326)
Trace[912791003]: [547.702017ms] [547.702017ms] END
I1204 12:47:45.027132       1 trace.go:236] Trace[2088147038]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:2917c603-dfd1-434a-9648-4ae556d5d2cb,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (04-Dec-2023 09:00:00.904) (total time: 3115ms):
Trace[2088147038]: ["GuaranteedUpdate etcd3" audit-id:2917c603-dfd1-434a-9648-4ae556d5d2cb,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 3115ms (09:00:00.904)
Trace[2088147038]:  ---"About to Encode" 111ms (09:00:01.016)
Trace[2088147038]:  ---"Txn call completed" 2995ms (12:47:45.018)]
Trace[2088147038]: [3.115141594s] [3.115141594s] END
I1204 12:47:45.148789       1 trace.go:236] Trace[1407869528]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:a4be8300-1c91-4403-b0e2-25c498d06599,client:192.168.49.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:PUT (04-Dec-2023 12:47:43.198) (total time: 2236ms):
Trace[1407869528]: ["GuaranteedUpdate etcd3" audit-id:a4be8300-1c91-4403-b0e2-25c498d06599,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 2235ms (12:47:43.198)
Trace[1407869528]:  ---"Txn call completed" 2234ms (12:47:45.148)]
Trace[1407869528]: [2.236178703s] [2.236178703s] END
I1204 12:47:45.205560       1 trace.go:236] Trace[643873345]: "Get" accept:application/json, */*,audit-id:d4c4f5ca-e095-41ad-bf5a-d5fe88f576bd,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (04-Dec-2023 09:00:01.108) (total time: 3089ms):
Trace[643873345]: ---"About to write a response" 3089ms (12:47:45.205)
Trace[643873345]: [3.089523812s] [3.089523812s] END
I1204 12:47:45.238727       1 trace.go:236] Trace[1587373986]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:18f7dfd1-624a-4da3-b704-dd49c18f79b8,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/kube-proxy/token,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (04-Dec-2023 12:47:43.786) (total time: 1452ms):
Trace[1587373986]: ---"Write to database call succeeded" len:148 1445ms (12:47:45.238)
Trace[1587373986]: [1.452016613s] [1.452016613s] END
I1204 12:47:45.252923       1 trace.go:236] Trace[1490992293]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d1cfccf7-d7b9-4624-8716-465563d11b28,client:192.168.49.2,protocol:HTTP/2.0,resource:serviceaccounts,scope:resource,url:/api/v1/namespaces/kube-system/serviceaccounts/storage-provisioner/token,user-agent:kubelet/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:POST (04-Dec-2023 12:47:43.655) (total time: 1597ms):
Trace[1490992293]: ---"Write to database call succeeded" len:151 1597ms (12:47:45.252)
Trace[1490992293]: [1.597645075s] [1.597645075s] END
I1204 12:47:45.401113       1 trace.go:236] Trace[1820513051]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-Dec-2023 09:00:00.902) (total time: 3491ms):
Trace[1820513051]: ---"initial value restored" 1746ms (12:47:43.655)
Trace[1820513051]: ---"Transaction prepared" 1548ms (12:47:45.204)
Trace[1820513051]: ---"Txn call completed" 196ms (12:47:45.401)
Trace[1820513051]: [3.49141099s] [3.49141099s] END
E1204 12:48:12.163328       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E1204 12:48:12.819226       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
I1204 12:48:42.678949       1 trace.go:236] Trace[1748427824]: "DeltaFIFO Pop Process" ID:v1.storage.k8s.io,Depth:19,Reason:slow event handlers blocking the queue (04-Dec-2023 12:48:42.470) (total time: 132ms):
Trace[1748427824]: [132.408721ms] [132.408721ms] END
I1204 12:48:44.223721       1 trace.go:236] Trace[1241534483]: "DeltaFIFO Pop Process" ID:v2.autoscaling,Depth:18,Reason:slow event handlers blocking the queue (04-Dec-2023 12:48:42.679) (total time: 1544ms):
Trace[1241534483]: [1.544208906s] [1.544208906s] END
I1204 12:48:45.586116       1 trace.go:236] Trace[1146447751]: "Get" accept:application/json, */*,audit-id:3fecece1-9fc5-4468-ae34-ff35793edddb,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (04-Dec-2023 12:48:42.305) (total time: 3230ms):
Trace[1146447751]: ---"About to write a response" 3229ms (12:48:45.584)
Trace[1146447751]: [3.230730762s] [3.230730762s] END
I1204 12:48:46.975749       1 trace.go:236] Trace[921852096]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-Dec-2023 12:48:41.965) (total time: 4960ms):
Trace[921852096]: ---"initial value restored" 4120ms (12:48:46.135)
Trace[921852096]: ---"Transaction prepared" 613ms (12:48:46.748)
Trace[921852096]: ---"Txn call completed" 226ms (12:48:46.975)
Trace[921852096]: [4.960788657s] [4.960788657s] END
I1204 12:50:12.910221       1 trace.go:236] Trace[848941715]: "Get" accept:application/json, */*,audit-id:8ed849f7-c774-48d7-a154-725871b4434a,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:GET (04-Dec-2023 12:50:11.828) (total time: 1058ms):
Trace[848941715]: ---"About to write a response" 994ms (12:50:12.908)
Trace[848941715]: [1.058920614s] [1.058920614s] END
I1204 12:50:14.688821       1 trace.go:236] Trace[1328453052]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.49.2,type:*v1.Endpoints,resource:apiServerIPInfo (04-Dec-2023 12:50:11.985) (total time: 2680ms):
Trace[1328453052]: ---"initial value restored" 289ms (12:50:12.275)
Trace[1328453052]: ---"Transaction prepared" 292ms (12:50:12.590)
Trace[1328453052]: ---"Txn call completed" 2097ms (12:50:14.688)
Trace[1328453052]: [2.680272689s] [2.680272689s] END
I1204 12:50:15.302696       1 trace.go:236] Trace[582448431]: "Update" accept:application/json, */*,audit-id:7fb4ca08-8d1c-423f-8ed1-0beac1002ee3,client:192.168.49.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (04-Dec-2023 12:50:13.104) (total time: 2198ms):
Trace[582448431]: ---"limitedReadBody succeeded" len:1356 32ms (12:50:13.136)
Trace[582448431]: ["GuaranteedUpdate etcd3" audit-id:7fb4ca08-8d1c-423f-8ed1-0beac1002ee3,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 2164ms (12:50:13.137)
Trace[582448431]:  ---"About to Encode" 114ms (12:50:13.252)
Trace[582448431]:  ---"Txn call completed" 2003ms (12:50:15.301)]
Trace[582448431]: [2.198171369s] [2.198171369s] END
I1204 12:50:15.606434       1 trace.go:236] Trace[653307893]: "Get" accept:application/vnd.kubernetes.protobuf, */*,audit-id:5e27f4d4-049b-49c2-86b1-0d2ec3fc96ee,client:127.0.0.1,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/default/endpoints/kubernetes,user-agent:kube-apiserver/v1.28.3 (linux/amd64) kubernetes/a8a1abc,verb:GET (04-Dec-2023 12:50:14.770) (total time: 836ms):
Trace[653307893]: ---"About to write a response" 835ms (12:50:15.606)
Trace[653307893]: [836.015112ms] [836.015112ms] END

* 
* ==> kube-controller-manager [52aad68fb2fa] <==
* I1211 15:16:38.672541       1 shared_informer.go:318] Caches are synced for PV protection
I1211 15:16:38.686245       1 shared_informer.go:318] Caches are synced for service account
I1211 15:16:38.692691       1 shared_informer.go:318] Caches are synced for ReplicationController
I1211 15:16:38.694307       1 shared_informer.go:318] Caches are synced for attach detach
I1211 15:16:38.711631       1 shared_informer.go:318] Caches are synced for ClusterRoleAggregator
I1211 15:16:38.714314       1 shared_informer.go:318] Caches are synced for deployment
I1211 15:16:38.718734       1 shared_informer.go:318] Caches are synced for GC
I1211 15:16:38.726517       1 shared_informer.go:318] Caches are synced for PVC protection
I1211 15:16:38.726622       1 shared_informer.go:318] Caches are synced for stateful set
I1211 15:16:38.733635       1 shared_informer.go:318] Caches are synced for taint
I1211 15:16:38.733852       1 node_lifecycle_controller.go:1225] "Initializing eviction metric for zone" zone=""
I1211 15:16:38.734112       1 node_lifecycle_controller.go:877] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1211 15:16:38.734252       1 node_lifecycle_controller.go:1071] "Controller detected that zone is now in new state" zone="" newState="Normal"
I1211 15:16:38.734329       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1211 15:16:38.734361       1 taint_manager.go:211] "Sending events to api server"
I1211 15:16:38.750098       1 shared_informer.go:318] Caches are synced for bootstrap_signer
I1211 15:16:38.750478       1 shared_informer.go:318] Caches are synced for TTL after finished
I1211 15:16:38.750695       1 shared_informer.go:318] Caches are synced for ReplicaSet
I1211 15:16:38.751192       1 shared_informer.go:318] Caches are synced for disruption
I1211 15:16:38.754489       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1211 15:16:38.755357       1 shared_informer.go:318] Caches are synced for endpoint
I1211 15:16:38.755675       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kube-system/coredns-5dd5756b68" duration="517.44s"
I1211 15:16:38.765302       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-7fd5cb4ddc" duration="9.476336ms"
I1211 15:16:38.765484       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-8694d4445c" duration="92.807s"
I1211 15:16:38.768676       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1211 15:16:38.776997       1 shared_informer.go:318] Caches are synced for cronjob
I1211 15:16:38.778512       1 shared_informer.go:318] Caches are synced for ephemeral
I1211 15:16:38.778593       1 shared_informer.go:318] Caches are synced for resource quota
I1211 15:16:38.781248       1 shared_informer.go:318] Caches are synced for job
I1211 15:16:38.781611       1 shared_informer.go:318] Caches are synced for HPA
I1211 15:16:38.796716       1 shared_informer.go:318] Caches are synced for daemon sets
I1211 15:16:38.815438       1 shared_informer.go:318] Caches are synced for resource quota
I1211 15:16:38.864288       1 shared_informer.go:318] Caches are synced for persistent volume
I1211 15:16:39.144632       1 event.go:307] "Event occurred" object="default/angular-service" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint default/angular-service: Operation cannot be fulfilled on endpoints \"angular-service\": the object has been modified; please apply your changes to the latest version and try again"
I1211 15:16:39.144733       1 event.go:307] "Event occurred" object="kubernetes-dashboard/dashboard-metrics-scraper" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kubernetes-dashboard/dashboard-metrics-scraper: Operation cannot be fulfilled on endpoints \"dashboard-metrics-scraper\": the object has been modified; please apply your changes to the latest version and try again"
I1211 15:16:39.144754       1 event.go:307] "Event occurred" object="kube-system/kube-dns" fieldPath="" kind="Endpoints" apiVersion="v1" type="Warning" reason="FailedToUpdateEndpoint" message="Failed to update endpoint kube-system/kube-dns: Operation cannot be fulfilled on endpoints \"kube-dns\": the object has been modified; please apply your changes to the latest version and try again"
I1211 15:16:39.162964       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="410.464358ms"
I1211 15:16:39.163502       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="218.717s"
I1211 15:16:39.234677       1 shared_informer.go:318] Caches are synced for garbage collector
I1211 15:16:39.234971       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1211 15:16:39.271704       1 shared_informer.go:318] Caches are synced for garbage collector
I1211 15:18:59.051772       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="22.498917ms"
I1211 15:18:59.051912       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="70.902s"
I1211 16:13:56.091186       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="43.170165ms"
I1211 16:13:56.175428       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="83.357211ms"
I1211 16:13:56.175533       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="52.8s"
I1211 16:13:57.272475       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="196.301s"
I1211 16:13:57.727491       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="209.101s"
I1211 16:13:57.819384       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="81.701s"
I1211 16:13:57.844549       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="65.8s"
I1211 16:13:57.914120       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="55.801s"
I1211 16:32:05.952517       1 event.go:307] "Event occurred" object="default/angular-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set angular-deployment-657b4d6b7b to 1"
I1211 16:32:05.998481       1 event.go:307] "Event occurred" object="default/angular-deployment-657b4d6b7b" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: angular-deployment-657b4d6b7b-7dst6"
I1211 16:32:06.019661       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="76.421985ms"
I1211 16:32:06.065914       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="46.165734ms"
I1211 16:32:06.066038       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="52.304s"
I1211 16:32:06.151188       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="87.606s"
I1211 16:32:06.209266       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="121.109s"
I1211 16:32:19.386408       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="21.683751ms"
I1211 16:32:19.386505       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/angular-deployment-657b4d6b7b" duration="48.199s"

* 
* ==> kube-controller-manager [7decaa387783] <==
* I1211 15:15:41.654490       1 serving.go:348] Generated self-signed cert in-memory
I1211 15:15:45.332029       1 controllermanager.go:189] "Starting" version="v1.28.3"
I1211 15:15:45.332106       1 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1211 15:15:45.338807       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1211 15:15:45.338817       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1211 15:15:45.341355       1 secure_serving.go:213] Serving securely on 127.0.0.1:10257
I1211 15:15:45.341942       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
E1211 15:16:04.910962       1 controllermanager.go:235] "Error building controller context" err="failed to wait for apiserver being healthy: timed out waiting for the condition: failed to get apiserver /healthz status: an error on the server (\"[+]ping ok\\n[+]log ok\\n[+]etcd ok\\n[+]poststarthook/start-kube-apiserver-admission-initializer ok\\n[+]poststarthook/generic-apiserver-start-informers ok\\n[+]poststarthook/priority-and-fairness-config-consumer ok\\n[+]poststarthook/priority-and-fairness-filter ok\\n[+]poststarthook/storage-object-count-tracker-hook ok\\n[+]poststarthook/start-apiextensions-informers ok\\n[+]poststarthook/start-apiextensions-controllers ok\\n[+]poststarthook/crd-informer-synced ok\\n[+]poststarthook/start-service-ip-repair-controllers ok\\n[-]poststarthook/rbac/bootstrap-roles failed: reason withheld\\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\\n[+]poststarthook/priority-and-fairness-config-producer ok\\n[+]poststarthook/start-system-namespaces-controller ok\\n[+]poststarthook/bootstrap-controller ok\\n[+]poststarthook/start-cluster-authentication-info-controller ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-controller ok\\n[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok\\n[+]poststarthook/start-legacy-token-tracking-controller ok\\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\\n[+]poststarthook/start-kube-aggregator-informers ok\\n[+]poststarthook/apiservice-registration-controller ok\\n[+]poststarthook/apiservice-status-available-controller ok\\n[+]poststarthook/kube-apiserver-autoregistration ok\\n[+]autoregister-completion ok\\n[+]poststarthook/apiservice-openapi-controller ok\\n[+]poststarthook/apiservice-openapiv3-controller ok\\n[+]poststarthook/apiservice-discovery-controller ok\\nhealthz check failed\") has prevented the request from succeeding"

* 
* ==> kube-proxy [669cf8ca44a0] <==
* I1123 14:24:47.373117       1 server_others.go:69] "Using iptables proxy"
E1123 14:24:47.472025       1 node.go:130] Failed to retrieve node info: Get "https://control-plane.minikube.internal:8443/api/v1/nodes/minikube": dial tcp 192.168.49.2:8443: connect: connection refused
I1123 14:24:59.493137       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1123 14:24:59.594536       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1123 14:24:59.600032       1 server_others.go:152] "Using iptables Proxier"
I1123 14:24:59.600363       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1123 14:24:59.600507       1 server_others.go:438] "Defaulting to no-op detect-local"
I1123 14:24:59.600720       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1123 14:24:59.601154       1 server.go:846] "Version info" version="v1.28.3"
I1123 14:24:59.602721       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1123 14:24:59.604434       1 config.go:188] "Starting service config controller"
I1123 14:24:59.604672       1 shared_informer.go:311] Waiting for caches to sync for service config
I1123 14:24:59.604925       1 config.go:97] "Starting endpoint slice config controller"
I1123 14:24:59.605177       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1123 14:24:59.605633       1 config.go:315] "Starting node config controller"
I1123 14:24:59.605831       1 shared_informer.go:311] Waiting for caches to sync for node config
I1123 14:24:59.705756       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1123 14:24:59.705795       1 shared_informer.go:318] Caches are synced for service config
I1123 14:24:59.707023       1 shared_informer.go:318] Caches are synced for node config
I1124 18:05:34.486626       1 trace.go:236] Trace[203688889]: "iptables ChainExists" (24-Nov-2023 18:05:27.090) (total time: 5177ms):
Trace[203688889]: [5.177641413s] [5.177641413s] END
I1124 18:05:34.486825       1 trace.go:236] Trace[592359437]: "iptables ChainExists" (24-Nov-2023 18:05:27.090) (total time: 5143ms):
Trace[592359437]: [5.14373099s] [5.14373099s] END
I1125 15:53:19.402556       1 trace.go:236] Trace[335393094]: "iptables ChainExists" (25-Nov-2023 15:53:15.581) (total time: 3821ms):
Trace[335393094]: [3.82120701s] [3.82120701s] END
I1125 15:53:19.716863       1 trace.go:236] Trace[995227802]: "iptables ChainExists" (25-Nov-2023 15:53:15.672) (total time: 4044ms):
Trace[995227802]: [4.044489246s] [4.044489246s] END
I1126 10:08:00.517140       1 trace.go:236] Trace[191996299]: "iptables save" (26-Nov-2023 09:54:10.415) (total time: 6362ms):
Trace[191996299]: [6.362024166s] [6.362024166s] END
I1127 12:05:22.737533       1 trace.go:236] Trace[110595571]: "iptables ChainExists" (26-Nov-2023 16:11:18.040) (total time: 10221ms):
Trace[110595571]: [10.221755135s] [10.221755135s] END
I1201 07:41:44.956094       1 trace.go:236] Trace[439323113]: "iptables ChainExists" (01-Dec-2023 07:41:41.445) (total time: 3506ms):
Trace[439323113]: [3.506849375s] [3.506849375s] END
I1201 07:41:44.985706       1 trace.go:236] Trace[1454223449]: "iptables ChainExists" (01-Dec-2023 07:41:41.625) (total time: 3360ms):
Trace[1454223449]: [3.360007145s] [3.360007145s] END
I1201 14:08:51.154533       1 trace.go:236] Trace[440757575]: "iptables ChainExists" (01-Dec-2023 14:08:41.834) (total time: 4245ms):
Trace[440757575]: [4.245449654s] [4.245449654s] END
I1201 14:08:52.440838       1 trace.go:236] Trace[167365625]: "iptables ChainExists" (01-Dec-2023 14:08:42.248) (total time: 10191ms):
Trace[167365625]: [10.191228712s] [10.191228712s] END
I1203 09:58:15.746820       1 trace.go:236] Trace[1385832785]: "iptables ChainExists" (03-Dec-2023 09:58:10.541) (total time: 2840ms):
Trace[1385832785]: [2.840281979s] [2.840281979s] END

* 
* ==> kube-proxy [eb4fe6aa0acf] <==
* I1211 15:16:14.184752       1 server_others.go:69] "Using iptables proxy"
I1211 15:16:14.923111       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I1211 15:16:16.275563       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1211 15:16:16.286113       1 server_others.go:152] "Using iptables Proxier"
I1211 15:16:16.286196       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I1211 15:16:16.286217       1 server_others.go:438] "Defaulting to no-op detect-local"
I1211 15:16:16.296223       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1211 15:16:16.297594       1 server.go:846] "Version info" version="v1.28.3"
I1211 15:16:16.297621       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1211 15:16:16.343295       1 config.go:188] "Starting service config controller"
I1211 15:16:16.349343       1 shared_informer.go:311] Waiting for caches to sync for service config
I1211 15:16:16.349522       1 config.go:97] "Starting endpoint slice config controller"
I1211 15:16:16.349537       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1211 15:16:16.352058       1 config.go:315] "Starting node config controller"
I1211 15:16:16.352081       1 shared_informer.go:311] Waiting for caches to sync for node config
I1211 15:16:16.552162       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1211 15:16:16.552260       1 shared_informer.go:318] Caches are synced for service config
I1211 15:16:16.553681       1 shared_informer.go:318] Caches are synced for node config

* 
* ==> kube-scheduler [0fe1e69c866a] <==
* E1123 14:24:48.484230       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.484431       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.484544       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.484704       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.484758       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.491680       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.491745       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.491876       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.491921       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.492051       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.492092       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.492211       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.492312       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.492429       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.492473       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.492587       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.492627       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.492741       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.492782       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.493799       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.493847       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.493966       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.494008       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.494669       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.494730       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.494857       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.494905       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:48.503062       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
E1123 14:24:48.503185       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.49.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
W1123 14:24:59.073988       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1123 14:24:59.074035       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1123 14:24:59.164343       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1123 14:24:59.164389       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W1123 14:24:59.164686       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1123 14:24:59.164706       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1123 14:24:59.164823       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1123 14:24:59.164841       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1123 14:24:59.164935       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1123 14:24:59.164963       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1123 14:24:59.165061       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1123 14:24:59.165075       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1123 14:24:59.165165       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1123 14:24:59.165179       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W1123 14:24:59.165254       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1123 14:24:59.165268       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1123 14:24:59.165357       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1123 14:24:59.165373       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W1123 14:24:59.165548       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1123 14:24:59.165574       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1123 14:24:59.165678       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1123 14:24:59.165692       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1123 14:24:59.165779       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1123 14:24:59.165793       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1123 14:24:59.165848       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1123 14:24:59.165861       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1123 14:24:59.165955       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1123 14:24:59.165979       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1123 14:24:59.166061       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1123 14:24:59.166075       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
I1123 14:25:00.882326       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [c6deb8439666] <==
* E1211 15:15:51.354860       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:51.355206       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:51.355255       1 trace.go:236] Trace[1615204394]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:41.353) (total time: 10000ms):
Trace[1615204394]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10000ms (15:15:51.355)
Trace[1615204394]: [10.000954925s] [10.000954925s] END
E1211 15:15:51.355273       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.49.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:51.425818       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:51.425898       1 trace.go:236] Trace[1497643385]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:41.423) (total time: 10002ms):
Trace[1497643385]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (15:15:51.425)
Trace[1497643385]: [10.002202862s] [10.002202862s] END
E1211 15:15:51.425994       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.49.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:51.456199       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:51.456314       1 trace.go:236] Trace[340887052]: "Reflector ListAndWatch" name:pkg/server/dynamiccertificates/configmap_cafile_content.go:206 (11-Dec-2023 15:15:41.453) (total time: 10001ms):
Trace[340887052]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:15:51.456)
Trace[340887052]: [10.001943153s] [10.001943153s] END
E1211 15:15:51.456425       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:51.655922       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:51.656027       1 trace.go:236] Trace[1274396709]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:41.653) (total time: 10001ms):
Trace[1274396709]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:15:51.655)
Trace[1274396709]: [10.001634696s] [10.001634696s] END
E1211 15:15:51.656057       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.49.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:51.819651       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:51.820221       1 trace.go:236] Trace[2115688349]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:41.815) (total time: 10004ms):
Trace[2115688349]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10003ms (15:15:51.819)
Trace[2115688349]: [10.004284142s] [10.004284142s] END
E1211 15:15:51.820567       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.49.2:8443/api/v1/nodes?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:52.094183       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:52.094264       1 trace.go:236] Trace[23502257]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:42.087) (total time: 10006ms):
Trace[23502257]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout 10006ms (15:15:52.094)
Trace[23502257]: [10.006320752s] [10.006320752s] END
E1211 15:15:52.097022       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:52.134909       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:52.134989       1 trace.go:236] Trace[686172827]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:42.133) (total time: 10001ms):
Trace[686172827]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:15:52.134)
Trace[686172827]: [10.001594381s] [10.001594381s] END
E1211 15:15:52.135006       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.49.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:52.297625       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:52.297779       1 trace.go:236] Trace[382442045]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:42.295) (total time: 10002ms):
Trace[382442045]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10002ms (15:15:52.297)
Trace[382442045]: [10.002588532s] [10.002588532s] END
E1211 15:15:52.297816       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.49.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:52.391281       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:52.391388       1 trace.go:236] Trace[1424272117]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:42.389) (total time: 10001ms):
Trace[1424272117]: ---"Objects listed" error:Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:15:52.391)
Trace[1424272117]: [10.001813826s] [10.001813826s] END
E1211 15:15:52.391408       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:54.373591       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
I1211 15:15:54.373729       1 trace.go:236] Trace[176122777]: "Reflector ListAndWatch" name:vendor/k8s.io/client-go/informers/factory.go:150 (11-Dec-2023 15:15:44.372) (total time: 10001ms):
Trace[176122777]: ---"Objects listed" error:Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout 10001ms (15:15:54.373)
Trace[176122777]: [10.001565812s] [10.001565812s] END
E1211 15:15:54.373752       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.49.2:8443/api/v1/services?limit=500&resourceVersion=0": net/http: TLS handshake timeout
W1211 15:15:55.155640       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1211 15:15:55.156967       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W1211 15:15:55.156779       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E1211 15:15:55.157131       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W1211 15:15:55.156821       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1211 15:15:55.157252       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W1211 15:15:55.156854       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1211 15:15:55.157352       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
I1211 15:15:55.643625       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Dec 11 15:15:58 minikube kubelet[1609]: I1211 15:15:58.370141    1609 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Dec 11 15:15:58 minikube kubelet[1609]: I1211 15:15:58.370592    1609 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Dec 11 15:15:59 minikube kubelet[1609]: I1211 15:15:59.400277    1609 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Dec 11 15:15:59 minikube kubelet[1609]: I1211 15:15:59.401816    1609 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Dec 11 15:16:03 minikube kubelet[1609]: E1211 15:16:03.190800    1609 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 11 15:16:03 minikube kubelet[1609]: E1211 15:16:03.191762    1609 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 11 15:16:04 minikube kubelet[1609]: I1211 15:16:04.757984    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6c2565361b5b90bc5eb5f170140a3c0060757c7665f23f4dee0b1ab28c26558d"
Dec 11 15:16:09 minikube kubelet[1609]: I1211 15:16:09.358689    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="55d672c11b688660105c800c4925271e7477de6304b48058858ead0d0579eef0"
Dec 11 15:16:10 minikube kubelet[1609]: I1211 15:16:10.014162    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="2e66f7dbdcffd1030278fd81987d2d2195f021032f7117e4b9f152b0b2631615"
Dec 11 15:16:10 minikube kubelet[1609]: I1211 15:16:10.033656    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="cad0d4ac6dc0e9d5d46bca562cc5636809228f5175e7d92505e6af2f35794bdb"
Dec 11 15:16:10 minikube kubelet[1609]: I1211 15:16:10.070805    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="06c8fd5df6ac05dff8db087f7154f9d332f8b0118165d4ce698a0f045144a7f5"
Dec 11 15:16:10 minikube kubelet[1609]: I1211 15:16:10.100367    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="323ffacc3517e3473611b47693f881fc10c45a0a2c5737d339116f856bb0a085"
Dec 11 15:16:13 minikube kubelet[1609]: I1211 15:16:13.800258    1609 scope.go:117] "RemoveContainer" containerID="72b7c5030eb8a9b2ed735509d8a87ae318e6b4b317c1869a46a93cc9b3a855fc"
Dec 11 15:16:13 minikube kubelet[1609]: I1211 15:16:13.801001    1609 scope.go:117] "RemoveContainer" containerID="7decaa3877834a032b1a7badac0449f3741f536e3ce09fcd85dffeaaa004b226"
Dec 11 15:16:13 minikube kubelet[1609]: E1211 15:16:13.802212    1609 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-controller-manager\" with CrashLoopBackOff: \"back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-minikube_kube-system(7da72fc2e2cfb27aacf6cffd1c72da00)\"" pod="kube-system/kube-controller-manager-minikube" podUID="7da72fc2e2cfb27aacf6cffd1c72da00"
Dec 11 15:16:15 minikube kubelet[1609]: I1211 15:16:15.162627    1609 scope.go:117] "RemoveContainer" containerID="7a5f48a8805fe6c6694c7678afad477f8cfd38281ecad65014da1c7916e7e8bf"
Dec 11 15:16:15 minikube kubelet[1609]: E1211 15:16:15.162942    1609 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(0eef51fb-db29-4049-bb68-e9c67dfe9b83)\"" pod="kube-system/storage-provisioner" podUID="0eef51fb-db29-4049-bb68-e9c67dfe9b83"
Dec 11 15:16:15 minikube kubelet[1609]: E1211 15:16:15.906141    1609 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
Dec 11 15:16:15 minikube kubelet[1609]: E1211 15:16:15.906467    1609 helpers.go:677] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
Dec 11 15:16:17 minikube kubelet[1609]: I1211 15:16:17.477968    1609 scope.go:117] "RemoveContainer" containerID="7decaa3877834a032b1a7badac0449f3741f536e3ce09fcd85dffeaaa004b226"
Dec 11 15:16:18 minikube kubelet[1609]: I1211 15:16:18.252604    1609 scope.go:117] "RemoveContainer" containerID="43ee0899fd0570118fda40d7afb9d23d88fe0f6cb83a156d890c0a749cebd5eb"
Dec 11 15:16:18 minikube kubelet[1609]: I1211 15:16:18.253412    1609 scope.go:117] "RemoveContainer" containerID="7a5f48a8805fe6c6694c7678afad477f8cfd38281ecad65014da1c7916e7e8bf"
Dec 11 15:16:18 minikube kubelet[1609]: E1211 15:16:18.253754    1609 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(0eef51fb-db29-4049-bb68-e9c67dfe9b83)\"" pod="kube-system/storage-provisioner" podUID="0eef51fb-db29-4049-bb68-e9c67dfe9b83"
Dec 11 15:16:33 minikube kubelet[1609]: I1211 15:16:33.292057    1609 scope.go:117] "RemoveContainer" containerID="7a5f48a8805fe6c6694c7678afad477f8cfd38281ecad65014da1c7916e7e8bf"
Dec 11 15:20:25 minikube kubelet[1609]: W1211 15:20:25.445032    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 15:25:25 minikube kubelet[1609]: W1211 15:25:25.444681    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 15:30:25 minikube kubelet[1609]: W1211 15:30:25.487325    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 15:35:25 minikube kubelet[1609]: W1211 15:35:25.458855    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 15:40:25 minikube kubelet[1609]: W1211 15:40:25.546468    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 15:45:25 minikube kubelet[1609]: W1211 15:45:25.558680    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 15:50:25 minikube kubelet[1609]: W1211 15:50:25.570523    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 15:55:25 minikube kubelet[1609]: W1211 15:55:25.575604    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:00:25 minikube kubelet[1609]: W1211 16:00:25.572670    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:05:25 minikube kubelet[1609]: W1211 16:05:25.565855    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:10:25 minikube kubelet[1609]: W1211 16:10:25.549510    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:13:57 minikube kubelet[1609]: I1211 16:13:57.313809    1609 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-sxnfz\" (UniqueName: \"kubernetes.io/projected/36e9a4c8-30b3-421f-98be-6c009aa7599a-kube-api-access-sxnfz\") pod \"36e9a4c8-30b3-421f-98be-6c009aa7599a\" (UID: \"36e9a4c8-30b3-421f-98be-6c009aa7599a\") "
Dec 11 16:13:57 minikube kubelet[1609]: I1211 16:13:57.368380    1609 operation_generator.go:882] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/36e9a4c8-30b3-421f-98be-6c009aa7599a-kube-api-access-sxnfz" (OuterVolumeSpecName: "kube-api-access-sxnfz") pod "36e9a4c8-30b3-421f-98be-6c009aa7599a" (UID: "36e9a4c8-30b3-421f-98be-6c009aa7599a"). InnerVolumeSpecName "kube-api-access-sxnfz". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 11 16:13:57 minikube kubelet[1609]: I1211 16:13:57.416572    1609 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-sxnfz\" (UniqueName: \"kubernetes.io/projected/36e9a4c8-30b3-421f-98be-6c009aa7599a-kube-api-access-sxnfz\") on node \"minikube\" DevicePath \"\""
Dec 11 16:13:57 minikube kubelet[1609]: I1211 16:13:57.658804    1609 scope.go:117] "RemoveContainer" containerID="cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f"
Dec 11 16:13:57 minikube kubelet[1609]: I1211 16:13:57.832291    1609 scope.go:117] "RemoveContainer" containerID="aef927e678d54e3de4665ce6f1aa76e869351d2379c40f4a0cec2a4e6e00897f"
Dec 11 16:13:58 minikube kubelet[1609]: I1211 16:13:58.011227    1609 scope.go:117] "RemoveContainer" containerID="cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f"
Dec 11 16:13:58 minikube kubelet[1609]: E1211 16:13:58.017168    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f" containerID="cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f"
Dec 11 16:13:58 minikube kubelet[1609]: I1211 16:13:58.017413    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f"} err="failed to get container status \"cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f\": rpc error: code = Unknown desc = Error response from daemon: No such container: cabd3431d781e3b694d124e7356e3b5d018fd6294d2361054e5a48b512a8f22f"
Dec 11 16:13:58 minikube kubelet[1609]: I1211 16:13:58.017444    1609 scope.go:117] "RemoveContainer" containerID="aef927e678d54e3de4665ce6f1aa76e869351d2379c40f4a0cec2a4e6e00897f"
Dec 11 16:13:58 minikube kubelet[1609]: E1211 16:13:58.018914    1609 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: aef927e678d54e3de4665ce6f1aa76e869351d2379c40f4a0cec2a4e6e00897f" containerID="aef927e678d54e3de4665ce6f1aa76e869351d2379c40f4a0cec2a4e6e00897f"
Dec 11 16:13:58 minikube kubelet[1609]: I1211 16:13:58.018976    1609 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"aef927e678d54e3de4665ce6f1aa76e869351d2379c40f4a0cec2a4e6e00897f"} err="failed to get container status \"aef927e678d54e3de4665ce6f1aa76e869351d2379c40f4a0cec2a4e6e00897f\": rpc error: code = Unknown desc = Error response from daemon: No such container: aef927e678d54e3de4665ce6f1aa76e869351d2379c40f4a0cec2a4e6e00897f"
Dec 11 16:13:59 minikube kubelet[1609]: I1211 16:13:59.400325    1609 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID="36e9a4c8-30b3-421f-98be-6c009aa7599a" path="/var/lib/kubelet/pods/36e9a4c8-30b3-421f-98be-6c009aa7599a/volumes"
Dec 11 16:15:25 minikube kubelet[1609]: W1211 16:15:25.570906    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:20:25 minikube kubelet[1609]: W1211 16:20:25.562033    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:25:25 minikube kubelet[1609]: W1211 16:25:25.568560    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:30:25 minikube kubelet[1609]: W1211 16:30:25.646194    1609 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Dec 11 16:32:06 minikube kubelet[1609]: I1211 16:32:06.153789    1609 topology_manager.go:215] "Topology Admit Handler" podUID="1a230885-350c-4e98-960c-c6f1f4df3999" podNamespace="default" podName="angular-deployment-657b4d6b7b-7dst6"
Dec 11 16:32:06 minikube kubelet[1609]: E1211 16:32:06.158768    1609 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="36e9a4c8-30b3-421f-98be-6c009aa7599a" containerName="angular"
Dec 11 16:32:06 minikube kubelet[1609]: E1211 16:32:06.158862    1609 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="36e9a4c8-30b3-421f-98be-6c009aa7599a" containerName="angular"
Dec 11 16:32:06 minikube kubelet[1609]: E1211 16:32:06.158888    1609 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="36e9a4c8-30b3-421f-98be-6c009aa7599a" containerName="angular"
Dec 11 16:32:06 minikube kubelet[1609]: I1211 16:32:06.161824    1609 memory_manager.go:346] "RemoveStaleState removing state" podUID="36e9a4c8-30b3-421f-98be-6c009aa7599a" containerName="angular"
Dec 11 16:32:06 minikube kubelet[1609]: I1211 16:32:06.161906    1609 memory_manager.go:346] "RemoveStaleState removing state" podUID="36e9a4c8-30b3-421f-98be-6c009aa7599a" containerName="angular"
Dec 11 16:32:06 minikube kubelet[1609]: I1211 16:32:06.223279    1609 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-w9dgv\" (UniqueName: \"kubernetes.io/projected/1a230885-350c-4e98-960c-c6f1f4df3999-kube-api-access-w9dgv\") pod \"angular-deployment-657b4d6b7b-7dst6\" (UID: \"1a230885-350c-4e98-960c-c6f1f4df3999\") " pod="default/angular-deployment-657b4d6b7b-7dst6"
Dec 11 16:32:07 minikube kubelet[1609]: I1211 16:32:07.982283    1609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="572625c6a1995eacefa57a082795ad72215f7faf5db13596866bde9c383406e3"
Dec 11 16:32:19 minikube kubelet[1609]: I1211 16:32:19.373260    1609 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="default/angular-deployment-657b4d6b7b-7dst6" podStartSLOduration=4.797195683 podCreationTimestamp="2023-12-11 16:32:05 +0000 UTC" firstStartedPulling="2023-12-11 16:32:08.151016651 +0000 UTC m=+4603.986223007" lastFinishedPulling="2023-12-11 16:32:17.71895684 +0000 UTC m=+4613.555278727" observedRunningTime="2023-12-11 16:32:19.365654013 +0000 UTC m=+4615.201975900" watchObservedRunningTime="2023-12-11 16:32:19.366251403 +0000 UTC m=+4615.202573290"

* 
* ==> kubernetes-dashboard [4662a5e8120c] <==
* 2023/11/23 14:25:11 Starting overwatch
I1126 10:07:58.412144       1 request.go:601] Waited for 1.013638472s due to client-side throttling, not priority and fairness, request: GET:https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-key-holder
2023/11/23 14:25:11 Using namespace: kubernetes-dashboard
2023/11/23 14:25:11 Using in-cluster config to connect to apiserver
2023/11/23 14:25:11 Using secret token for csrf signing
2023/11/23 14:25:11 Initializing csrf token from kubernetes-dashboard-csrf secret
2023/11/23 14:25:11 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2023/11/23 14:25:11 Successful initial request to the apiserver, version: v1.28.3
2023/11/23 14:25:11 Generating JWE encryption key
2023/11/23 14:25:11 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2023/11/23 14:25:11 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2023/11/23 14:25:11 Initializing JWE encryption key from synchronized object
2023/11/23 14:25:11 Creating in-cluster Sidecar client
2023/11/23 14:25:11 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2023/11/23 14:25:11 Serving insecurely on HTTP port: 9090
2023/11/23 14:25:41 Successful request to sidecar

* 
* ==> kubernetes-dashboard [ebba4b4f43e1] <==
* 2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 Getting list of all jobs in the cluster
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 Getting list of all cron jobs in the cluster
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 Getting list of all deployments in the cluster
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 Getting list of all pods in the cluster
2023/12/11 16:23:55 Getting pod metrics
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 Getting list of all replica sets in the cluster
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 Getting list of all pet sets in the cluster
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:23:55 Getting list of all replication controllers in the cluster
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:23:55 [2023-12-11T16:23:55Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 Getting list of all pods in the cluster
2023/12/11 16:24:00 Getting list of all jobs in the cluster
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/11 16:24:00 Getting list of namespaces
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 Getting list of all deployments in the cluster
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 Getting list of all cron jobs in the cluster
2023/12/11 16:24:00 Getting pod metrics
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 Getting list of all replication controllers in the cluster
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 Getting list of all replica sets in the cluster
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:00 Getting list of all pet sets in the cluster
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:00 [2023-12-11T16:24:00Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:01 [2023-12-11T16:24:01Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:01 Getting list of all cron jobs in the cluster
2023/12/11 16:24:01 [2023-12-11T16:24:01Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:01 [2023-12-11T16:24:01Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2023/12/11 16:24:01 Getting list of namespaces
2023/12/11 16:24:01 [2023-12-11T16:24:01Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2023/12/11 16:24:01 Getting list of all deployments in the cluster
2023/12/11 16:24:01 [2023-12-11T16:24:01Z] Outcoming response to 127.0.0.1 with 200 status code
2023/12/11 16:24:01 [2023-12-11T16:24:01Z] Outcoming response to 127.0.0.1 with 200 status code

* 
* ==> storage-provisioner [67b4370788b2] <==
* I1211 15:16:34.444802       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1211 15:16:34.511629       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1211 15:16:34.522257       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1211 15:16:51.968807       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1211 15:16:51.968984       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_105dbf45-8728-4195-bcb8-26795cc62470!
I1211 15:16:51.969032       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"10a8989b-ddc3-45ae-8283-e7dfbae58906", APIVersion:"v1", ResourceVersion:"233819", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_105dbf45-8728-4195-bcb8-26795cc62470 became leader
I1211 15:16:52.069603       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_105dbf45-8728-4195-bcb8-26795cc62470!

* 
* ==> storage-provisioner [7a5f48a8805f] <==
* I1211 15:16:12.058230       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1211 15:16:12.498175       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority

